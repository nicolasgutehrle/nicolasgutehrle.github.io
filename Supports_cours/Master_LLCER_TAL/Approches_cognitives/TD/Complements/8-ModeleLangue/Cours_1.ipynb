{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modèle de langue et word embeddings\n",
    "\n",
    "Les modèles de langue sont des modèles probabilistes d'une langue. A partir d'un corpus d'étude, ils apprennent la probabilité d'apparition d'un terme, ainsi que la probabilité qu'un terme suive un autre terme ou séquence de termes. Ainsi, les modèles de langues appartiennent à la famille des méthodes non-supervisées. Ils sont particulièrement adapté pour étudier la structure d'une langue, mais aussi adaptés à des tâches comme la reconnaissance de langue, l'analyse en partie du discours ou la génération de texte. \n",
    "\n",
    "Les premiers modèles de language reposent sur des méthodes statistiques. Plus récémment, les principaux modèles de langues tels que BERT ou GPT reposent sur les méthodes neuronales. Si ces derniers sont bien plus puissants et efficaces, les ressources informatiques nécessaires pour les faire tourner sont bien plus importantes.  \n",
    "\n",
    "Dans ce cours, nous allons entraîner un modèle de langue statistique à l'aide de la librairie NLTK. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading regex-2023.8.8-cp39-cp39-macosx_10_9_x86_64.whl (294 kB)\n",
      "\u001b[K     |████████████████████████████████| 294 kB 6.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: joblib in /Users/nicolasgutehrle/opt/anaconda3/envs/cours/lib/python3.9/site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: tqdm in /Users/nicolasgutehrle/opt/anaconda3/envs/cours/lib/python3.9/site-packages (from nltk) (4.62.3)\n",
      "Requirement already satisfied: click in /Users/nicolasgutehrle/opt/anaconda3/envs/cours/lib/python3.9/site-packages (from nltk) (8.1.7)\n",
      "Installing collected packages: regex, nltk\n",
      "Successfully installed nltk-3.8.1 regex-2023.8.8\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modèle n-gramme\n",
    "\n",
    "Un modèle n-gramme détermine le mot suivant le plus probable à partir d'une séquence n-1 de mots. Par exemple, un modèle bigramme déterminera la probabilité qu'un mot apparaisse en fonction du mot précédent, un trigramme à partir des deux mots précédents, et ainsi de suite. Un modèle unigramme lui ne repose que sur la probabilité d'apparition du mot dans le corpus, sans prendre en compte ce qui précéde. \n",
    "\n",
    "Exemple : \n",
    "\n",
    "p(There was heavy rainfall) = p(START, There, was, heavy, rainfall, END) = p(There|START)p(was|There)p(heavy|There was)p(rainfall|There was heavy)p(END|There was heavy rainfall)\n",
    "\n",
    "Cependant, on ne peut réellement calculer la probabilité d'apparition d'une séquence aussi longue. On peut cependant calculer les probabilités d'apparition de chaque n-gramme précédents. Ainsi : \n",
    "\n",
    "p(There was heavy rainfall) = p(There|START)p(was|There)p(heavy|was)p(rainfall|heavy)p(END|rainfall)\n",
    "\n",
    "Pour la génération de texte, les modèles à partir de trigrammes sont les plus intéressants. Cependant, la taille du modèle dépend de la tâche et du type de données traité."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Préparer les données\n",
    "\n",
    "Pour entraîner un modèle, il nous faut des liste de documents tokénisé. Ci-dessous, la variable \"corpus\" contient deux \"phrases\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    ['a', 'b', 'c'],\n",
    "    ['a', 'c', 'd', 'c', 'e', 'f']\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK permet de rapidement constituer des n-grammes. Ci-dessous, on produit des bigrammes à partir de nos séquences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigrams\n",
      "[('a', 'b'), ('b', 'c')]\n",
      "[('a', 'c'), ('c', 'd'), ('d', 'c'), ('c', 'e'), ('e', 'f')]\n",
      "Trigrams\n",
      "[('a', 'b', 'c')]\n",
      "[('a', 'c', 'd'), ('c', 'd', 'c'), ('d', 'c', 'e'), ('c', 'e', 'f')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import bigrams, trigrams\n",
    "\n",
    "print('Bigrams')\n",
    "for x in corpus:\n",
    "    gram = list(bigrams(x))\n",
    "    print(gram)\n",
    "\n",
    "print('Trigrams')\n",
    "for x in corpus:\n",
    "    gram = list(trigrams(x))\n",
    "    print(gram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est important de distinguer l'occurrence d'un mot au sein de la phrase d'au début ou de la fin de phrase. Pour cela, on ajoute à la séquence deux caractères spéciaux, **s** et **/s** pour indiquer respectivement le début et la fin de phrase. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'a', 'b', 'c', '</s>']\n",
      "['<s>', 'a', 'c', 'd', 'c', 'e', 'f', '</s>']\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import pad_sequence\n",
    "\n",
    "for x in corpus:\n",
    "    # l'argument n précise que l'on travaille sur des bigrammes\n",
    "    seq = pad_sequence(x, n=2, pad_left=True, pad_right=True, left_pad_symbol='<s>', right_pad_symbol='</s>')\n",
    "    print(list(seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction \"pad_both_ends\" facilite l'emploi de \"pad_sequence\". Ci-dessous, une fonction pour prétraiter un corpus. Notez l'argument \"gram_func\" qui attend une fonction pour transformer la séquence en n-gramme (ex: fonction bigrams ou trigrams de NLTK). L'argument \"n\" doit correspondre au n-gramme donné."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'a', 'b', 'c', '</s>']\n",
      "['<s>', 'a', 'c', 'd', 'c', 'e', 'f', '</s>']\n"
     ]
    }
   ],
   "source": [
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "\n",
    "for x in corpus:\n",
    "    # l'argument n précise que l'on travaille sur des bigrammes\n",
    "    seq = pad_both_ends(x, n=2)\n",
    "    print(list(seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afin d'avoir un modèle plus robuste, on peut l'entraîner sur des bigrammes et unigrammes à la fois. NLTK met à disposition la fonction \"everygram\", qui génère tous les n-grammes d'une séquence jusqu'à n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<s>',), ('<s>', '<s>'), ('<s>', '<s>', 'a'), ('<s>',), ('<s>', 'a'), ('<s>', 'a', 'b'), ('a',), ('a', 'b'), ('a', 'b', 'c'), ('b',), ('b', 'c'), ('b', 'c', '</s>'), ('c',), ('c', '</s>'), ('c', '</s>', '</s>'), ('</s>',), ('</s>', '</s>'), ('</s>',)]\n",
      "[('<s>',), ('<s>', '<s>'), ('<s>', '<s>', 'a'), ('<s>',), ('<s>', 'a'), ('<s>', 'a', 'c'), ('a',), ('a', 'c'), ('a', 'c', 'd'), ('c',), ('c', 'd'), ('c', 'd', 'c'), ('d',), ('d', 'c'), ('d', 'c', 'e'), ('c',), ('c', 'e'), ('c', 'e', 'f'), ('e',), ('e', 'f'), ('e', 'f', '</s>'), ('f',), ('f', '</s>'), ('f', '</s>', '</s>'), ('</s>',), ('</s>', '</s>'), ('</s>',)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import everygrams\n",
    "n = 3\n",
    "for x in corpus:\n",
    "    # l'argument n précise que l'on travaille sur des bigrammes\n",
    "    padded_seq = pad_both_ends(x, n=n)\n",
    "    padded_grams = list(everygrams(padded_seq, max_len=n))\n",
    "    print(padded_grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le modèle nécessite également un vocabulaire, c'est à dire d'un ensemble de mots connus du modèle. Les symboles de début et fin de phrases font partie de ce vocabulaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', 'a', 'b', 'c', '</s>', '<s>', 'a', 'c', 'd', 'c', 'e', 'f', '</s>']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = [token for sent in text for token in pad_both_ends(sent, n=2)]\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction \"padded_everygram_pipeline\" nous permet de procéder à toutes ces étapes à l'aide d'une seule fonction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "n = 2\n",
    "data, vocab = padded_everygram_pipeline(n, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<s>',)\n",
      "('<s>', 'a')\n",
      "('a',)\n",
      "('a', 'b')\n",
      "('b',)\n",
      "('b', 'c')\n",
      "('c',)\n",
      "('c', '</s>')\n",
      "('</s>',)\n",
      "('<s>',)\n",
      "('<s>', 'a')\n",
      "('a',)\n",
      "('a', 'c')\n",
      "('c',)\n",
      "('c', 'd')\n",
      "('d',)\n",
      "('d', 'c')\n",
      "('c',)\n",
      "('c', 'e')\n",
      "('e',)\n",
      "('e', 'f')\n",
      "('f',)\n",
      "('f', '</s>')\n",
      "('</s>',)\n"
     ]
    }
   ],
   "source": [
    "for x in data:\n",
    "    for y in x:\n",
    "        print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>\n",
      "a\n",
      "b\n",
      "c\n",
      "</s>\n",
      "<s>\n",
      "a\n",
      "c\n",
      "d\n",
      "c\n",
      "e\n",
      "f\n",
      "</s>\n"
     ]
    }
   ],
   "source": [
    "for x in vocab:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entraîner le modèle\n",
    "\n",
    "Nous pouvons désormais entraîner un modèle de langue. Nous allons entraîner un modèle MLE (Maximum Likelihood Estimator), qui compte la fréquence de chaque n-gramme puis les normalise pour que ces valeurs soient contenues entre 0 et 1. Pour plus de détails sur le fonction, voir Jurafsky. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.lm import MLE\n",
    "n = 2\n",
    "lm = MLE(n)\n",
    "# le vocabulaire est vide au départ\n",
    "len(lm.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.fit(data, vocab)\n",
    "len(lm.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/g9/npfr2mks4118dkv7g8ckgccr0000gn/T/ipykernel_68605/1042022009.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('a', 'b', 'c')\n",
      "('<UNK>', '<UNK>', '<UNK>')\n"
     ]
    }
   ],
   "source": [
    "print(lm.vocab.lookup(corpus[0]))\n",
    "# les mots \"aliens\", \"from\", \"Mars\" ne sont pas contenus dans le vocabulaire. Ils sont donc associés à un token UNK\n",
    "print(lm.vocab.lookup([\"aliens\", \"from\", \"Mars\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compte de \"a\"  2\n",
      "0.15384615384615385\n",
      "0.5\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# \"a\" apparait 2 fois dans le corpus\n",
    "print('Compte de \"a\" ', lm.counts['a'])\n",
    "\n",
    "# la probabilité que \"a\" apparaîsse dans le corpus\n",
    "print(lm.score(\"a\"))\n",
    "\n",
    "# la probabilité que \"b\" apparaîsse en étant précédé de \"a\" == 1/2\n",
    "print(lm.score(\"b\", [\"a\"]))\n",
    "\n",
    "# la probabilité que \"d\" apparaîsse en étant précédé de \"a\" == aucune\n",
    "print(lm.score(\"d\", [\"a\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluer un modèle de langue\n",
    "\n",
    "Comme pour l'apprentissage supervisé, il nous faut des données de test pour évaluer un modèle. Cependant, le modèle de langue ne peut s'évaluer en terme de Précision, Rappel et F1, puisqu'il ne s'agit ni de classer, ni de retourner des informations. \n",
    "\n",
    "Pour évaluer un modèle de langue, on se sert de la mesure de **perplexité**, c'est à dire la probabilité ou la surprise que les données de test correspondent au modèle de langue. Elle repose sur la notion d'entropie, c'est-à-dire d'incertitude, dans la théorie de l'information. Plus la perplexité est basse (donc la probabilité est haute), et plus les données de test correspondent. Ainsi, pour évaluer un modèle de langue, il nous faut des extraits de langues qui sont représentatifs. \n",
    "\n",
    "La perplexité n'est pas la seule mesure pour évaluer un modèle de langue, mais s'en est une des principales. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.449489742783178\n",
      "inf\n",
      "inf\n"
     ]
    }
   ],
   "source": [
    "# ces données correspondent à notre modèle\n",
    "test = [('a', 'b'), ('c', 'd')]\n",
    "print(lm.perplexity(test))\n",
    "\n",
    "# ces données ne correspondent pas à notre modèle: le modèle est inf(iniment) surpris\n",
    "test = [('a', 'c'), ('b', 'd')]\n",
    "print(lm.perplexity(test))\n",
    "\n",
    "test = [('1', '2'), ('3', '4')]\n",
    "print(lm.perplexity(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Générer un texte\n",
    "\n",
    "La force majeure des modèles de langues est de pouvoir générer des séquences de tokens (mots, POS tags...). On peut laisser le modèle tout générer seul ou bien lui donner une séquence de départ. De plus, on peut intégrer des règles pour modifier le processus de génération de texte. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c', '</s>', '<s>', 'a', 'c']\n",
      "['a', 'b', 'c', '</s>', 'c']\n",
      "['c', '</s>', '<s>', 'a', 'c']\n"
     ]
    }
   ],
   "source": [
    "# génération simple\n",
    "print(lm.generate(5, random_seed=42))\n",
    "\n",
    "# génération en spécifiant le début de la séquence\n",
    "print(lm.generate(5, text_seed=['<s>'],random_seed=42))\n",
    "\n",
    "print(lm.generate(5, text_seed=['<s>', 'a'],random_seed=42))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoothing\n",
    "\n",
    "Pour le moment, notre modèle n'est capable d'assigner une probabilité à une séquence seulement s'il l'a vu dans le corpus d'étude. Cependant, un caractère peut être présent dans le vocabulaire, mais apparaître dans un nouveau contexte dans un autre corpus. Un caractère peut également ne pas apparaître du tout dans le vocabulaire du modèle. Dans un tel cas, ce caractère aura une probabilité nulle qui lui sera assignée. Pour éviter cela, il faut procéder au **smoothing**. Les principaux algorithmes de smoothing sont :\n",
    "\n",
    "* Laplace (add-one) smoothing\n",
    "* add-k smoothing\n",
    "* stupid backoff\n",
    "* Kneser-Ney smoothing\n",
    "\n",
    "Une autre possibilité est d'employer l'**interpolation** ou le **backoff**\n",
    "\n",
    "* backoff : si une séquence n'existe pas, on réduit cette séquence jusqu'à en trouver une connue du modèle\n",
    "* interpolation : on fait la somme de la probabilité associée à chaque n-gramme contenu dans la séquence (unigrame + bigramme + ... + n-gramme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm import Laplace\n",
    "n = 2\n",
    "\n",
    "data, vocab = padded_everygram_pipeline(n, corpus)\n",
    "\n",
    "lm = Laplace(n)\n",
    "lm.fit(data, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.744562646538029\n",
      "7.416198487095664\n",
      "9.000000000000002\n"
     ]
    }
   ],
   "source": [
    "# ces données correspondent à notre modèle\n",
    "test = [('a', 'b'), ('c', 'd')]\n",
    "print(lm.perplexity(test))\n",
    "\n",
    "# ces données ne correspondent pas à notre modèle: le modèle est inf(iniment) surpris\n",
    "test = [('a', 'c'), ('b', 'd')]\n",
    "print(lm.perplexity(test))\n",
    "\n",
    "test = [('1', '2'), ('3', '4')]\n",
    "print(lm.perplexity(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm import AbsoluteDiscountingInterpolated\n",
    "n = 2\n",
    "\n",
    "data, vocab = padded_everygram_pipeline(n, corpus)\n",
    "\n",
    "lm = AbsoluteDiscountingInterpolated(order=n)\n",
    "lm.fit(data, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.2300398978808\n",
      "7.62564998111037\n",
      "inf\n"
     ]
    }
   ],
   "source": [
    "# ces données correspondent à notre modèle\n",
    "test = [('a', 'b'), ('c', 'd')]\n",
    "print(lm.perplexity(test))\n",
    "\n",
    "# ces données ne correspondent pas à notre modèle: le modèle est inf(iniment) surpris\n",
    "test = [('a', 'c'), ('b', 'd')]\n",
    "print(lm.perplexity(test))\n",
    "\n",
    "test = [('1', '2'), ('3', '4')]\n",
    "print(lm.perplexity(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ressources : \n",
    "\n",
    "Pour plus de détails dans les calculs de probabilités et la construction mathématique des modèles de langue : \n",
    "* Documentation NLTK : https://www.nltk.org/api/nltk.lm.html\n",
    "* Jurafsky et al : https://web.stanford.edu/~jurafsky/slp3/3.pdf"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOqsFrkTED839+eY31PFksJ",
   "name": "Cours.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
