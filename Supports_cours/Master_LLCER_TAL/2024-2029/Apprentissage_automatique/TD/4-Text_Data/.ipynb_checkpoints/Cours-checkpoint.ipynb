{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Donnée textuelles\n",
    "\n",
    "Comme nous l'avons vu précédemment, les algorithmes utilisés en ML demandent que les entrées soient sous formes numérques, qu'il s'agisse des données elles-mêmes ou des classes. Pour cette raison, nous avons dans le cours précédent encodés numériquement les classes de variété d'iris, tout en créant un mapping permettant de faire correspondre le nom de chaque variété à sa forme numérique.\n",
    "\n",
    "Cependant, s'il est relativement aisé d'encoder un ensemble de noms de classes, qui sont alors en nombre fini, quand est-il de données textuelles brutes, tel que des avis, des romans ou des articles ? Comment peut-on représenter la sémantique ou la syntaxe d'un texte de manière numérique ? Une grande partie des recherches en TAL (ou NLP) du point de vue du Machine Learning consiste à trouver de nouvelles méthodes pour répondre à cette problématique.\n",
    "\n",
    "Dans ce cours, nous allons voir les premières méthodes utilisés pour encoder des données textuelles, ce qui nous permettra de traiter un problème d'analyse de sentiment, qui est un cas récurrent de classification en TAL.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons travailler à partir de la version française du ``amazon_reviews`` dataset. Contrairement aux datasets que nous avons utilisés précédemment, celui-ci est déjà divisé en train, validation et test sets. Ceux-ci sont enregistrés au format JSON, mais nous pouvons tout de même les ouvrir avec ``pandas``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "trainset = pd.read_json('data/amazon_reviews/train/dataset_fr_train.json', lines=True)\n",
    "devset = pd.read_json('data/amazon_reviews/dev/dataset_fr_dev.json', lines=True)\n",
    "testset = pd.read_json('data/amazon_reviews/test/dataset_fr_test.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>reviewer_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>review_body</th>\n",
       "      <th>review_title</th>\n",
       "      <th>language</th>\n",
       "      <th>product_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fr_0424335</td>\n",
       "      <td>product_fr_0297678</td>\n",
       "      <td>reviewer_fr_0961886</td>\n",
       "      <td>1</td>\n",
       "      <td>A déconseiller - Article n'a fonctionné qu'une...</td>\n",
       "      <td>Brumisateur à pompe</td>\n",
       "      <td>fr</td>\n",
       "      <td>beauty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fr_0452615</td>\n",
       "      <td>product_fr_0613288</td>\n",
       "      <td>reviewer_fr_0857499</td>\n",
       "      <td>1</td>\n",
       "      <td>Si vous voulez être déçu achetez le produit ! ...</td>\n",
       "      <td>Insatisfaisant</td>\n",
       "      <td>fr</td>\n",
       "      <td>baby_product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fr_0407673</td>\n",
       "      <td>product_fr_0571250</td>\n",
       "      <td>reviewer_fr_0383240</td>\n",
       "      <td>1</td>\n",
       "      <td>Écran de mauvaise qualité, car il s'use en peu...</td>\n",
       "      <td>Ne recommande pas</td>\n",
       "      <td>fr</td>\n",
       "      <td>pc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fr_0579191</td>\n",
       "      <td>product_fr_0030168</td>\n",
       "      <td>reviewer_fr_0729693</td>\n",
       "      <td>1</td>\n",
       "      <td>Cet engin ne sert à rien les sons sont pourris...</td>\n",
       "      <td>A éviter!</td>\n",
       "      <td>fr</td>\n",
       "      <td>musical_instruments</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fr_0931533</td>\n",
       "      <td>product_fr_0468261</td>\n",
       "      <td>reviewer_fr_0734066</td>\n",
       "      <td>1</td>\n",
       "      <td>Très beau produit mais la grue n'a pas fonctio...</td>\n",
       "      <td>Déçue</td>\n",
       "      <td>fr</td>\n",
       "      <td>toy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    review_id          product_id          reviewer_id  stars  \\\n",
       "0  fr_0424335  product_fr_0297678  reviewer_fr_0961886      1   \n",
       "1  fr_0452615  product_fr_0613288  reviewer_fr_0857499      1   \n",
       "2  fr_0407673  product_fr_0571250  reviewer_fr_0383240      1   \n",
       "3  fr_0579191  product_fr_0030168  reviewer_fr_0729693      1   \n",
       "4  fr_0931533  product_fr_0468261  reviewer_fr_0734066      1   \n",
       "\n",
       "                                         review_body         review_title  \\\n",
       "0  A déconseiller - Article n'a fonctionné qu'une...  Brumisateur à pompe   \n",
       "1  Si vous voulez être déçu achetez le produit ! ...       Insatisfaisant   \n",
       "2  Écran de mauvaise qualité, car il s'use en peu...    Ne recommande pas   \n",
       "3  Cet engin ne sert à rien les sons sont pourris...            A éviter!   \n",
       "4  Très beau produit mais la grue n'a pas fonctio...                Déçue   \n",
       "\n",
       "  language     product_category  \n",
       "0       fr               beauty  \n",
       "1       fr         baby_product  \n",
       "2       fr                   pc  \n",
       "3       fr  musical_instruments  \n",
       "4       fr                  toy  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce dataset regroupe les avis laissés par les acheteurs sur Amazon. Il est intéressant puisque chaque avis est associé à plusieurs catégories, ce qui permet d'utiliser ce dataset pour différents problèmes. Ici, la note laissée par l'acheteur pour un produit est indiquée dans la colonne 'stars' et exprime sa satisfaction quant au produit. \n",
    "\n",
    "Pour résoudre notre problématique d'analyse de sentiments, nous n'allons garder que les colonnes review_body et stars, que nous renommerons 'texts' et 'classes'. Il nous faudra appliquer ce filtre aux trois datasets (train, dev et test). Nous allons donc écrire une fonction d'appliquer le même filtrage facilement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_df(df, list_columns):\n",
    "    \"\"\"\n",
    "    Retourne le DataFrame avec uniquement les colonnes demandées\n",
    "    \"\"\"\n",
    "    filtered_df = df[list_columns]\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_body</th>\n",
       "      <th>stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A déconseiller - Article n'a fonctionné qu'une...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Si vous voulez être déçu achetez le produit ! ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Écran de mauvaise qualité, car il s'use en peu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cet engin ne sert à rien les sons sont pourris...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Très beau produit mais la grue n'a pas fonctio...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         review_body  stars\n",
       "0  A déconseiller - Article n'a fonctionné qu'une...      1\n",
       "1  Si vous voulez être déçu achetez le produit ! ...      1\n",
       "2  Écran de mauvaise qualité, car il s'use en peu...      1\n",
       "3  Cet engin ne sert à rien les sons sont pourris...      1\n",
       "4  Très beau produit mais la grue n'a pas fonctio...      1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keep_col = ['review_body', 'stars'] # liste des colonnes que l'on souhaite garder\n",
    "train = filter_df(trainset, keep_col) \n",
    "dev = filter_df(devset, keep_col)\n",
    "test = filter_df(testset, keep_col)\n",
    "train.head() # on visualise les premieres lignes du df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons créer une fonction pour renommer automatiquement les colonnes de la même façon sur les trois datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_columns(df, list_rename):\n",
    "    \"\"\"\n",
    "    Retourne un DF avec les noms de colonnes changées par celles de list_rename\n",
    "    \"\"\"\n",
    "    df.columns = list_rename\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texts</th>\n",
       "      <th>classes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A déconseiller - Article n'a fonctionné qu'une...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Si vous voulez être déçu achetez le produit ! ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Écran de mauvaise qualité, car il s'use en peu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cet engin ne sert à rien les sons sont pourris...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Très beau produit mais la grue n'a pas fonctio...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               texts  classes\n",
       "0  A déconseiller - Article n'a fonctionné qu'une...        1\n",
       "1  Si vous voulez être déçu achetez le produit ! ...        1\n",
       "2  Écran de mauvaise qualité, car il s'use en peu...        1\n",
       "3  Cet engin ne sert à rien les sons sont pourris...        1\n",
       "4  Très beau produit mais la grue n'a pas fonctio...        1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_rename = ['texts', 'classes']\n",
    "train = rename_columns(train, list_rename)\n",
    "dev = rename_columns(dev, list_rename)\n",
    "test = rename_columns(test, list_rename)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation des classes\n",
    "\n",
    "Comme on peut le voir ci-dessus, les classes sont déjà sous forme numérique: il n'y a donc pas besoin d'utiliser le ``LabelEncoder`` de ``scikit-learn`` pour transformer les noms des classes. Cependant, il est toujours important de visualiser la distribution des classes, afin de s'assurer que certaines classes soient déséquilibrées:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice\n",
    "\n",
    "Produisez un diagramme à barres pour visualiser la répartition de chaque classes, puis interprétez le résultat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exercice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train.classes.value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec le diagramme ci-dessus, on constate qu'il y a 40 000 exemples pour chaque classe, ce qui fait qu'elles sont équilibrées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "\n",
    "Comme nous l'avons vu au début de ce cours, chaque dataset est composé de ``samples`` (lignes) et de ``features`` (colonnes). Ces features doivent être au format numérique pour pouvoir être utilisées par les algorithmes. Cependant, notre colonne ``text`` n'est pas en soit une feature puisqu'on ne peut l'utiliser telle quel.  Il faut donc trouver un moyen d'extraire les caractéristiques de cette colonne. En Machine-Learning, cette étape est appelée ``Feature-Extraction``, et est une étape cruciale dans le processus d'entraînement d'un modèle.\n",
    "\n",
    "Feature-extraction ne concerne pas que le traitement des données textuelles: elle concerne le traitement de n'importe quel type de features. On peut ainsi répartir les valeurs d'une colonne en trois colonnes ou à l'inverse, réduire le nombre de colonnes utilisées. A DVLP\n",
    "\n",
    "https://towardsdatascience.com/feature-extraction-techniques-d619b56e31be"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-Words\n",
    "\n",
    "Une des plus simples méthodes de Feature-Extraction pour le texte est la méthode dite du ``Bag-of-Words``. Cette méthode ne s'intéresse qu'aux mots eux-mêmes, sans se préoccuper des paragraphes ou de la syntaxe. Elle consiste pour un ensemble de documents donné à compter le nombre d'occurrence de chaque mot. En se débarassant de la syntaxe ou de l'organisation des textes, on consitue donc un ``sac de mots`` contenant un vocabulaire, c'est-à-dire un ensemble fini de mots. On peut alors représenter chaque document par le nombre d'occurrence de chaque mot qu'il contient.\n",
    "\n",
    "Etablir un sac de mots se réalise en trois étapes:\n",
    "\n",
    "* La tokenization\n",
    "* La constitution du vocabulaire\n",
    "* L'encodage\n",
    "\n",
    "Pour démontrer ces trois étapes, nous allons utiliser le faux corpus ci-dessous:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ['Un anneau pour les gouverner tous.', \n",
    "          'Un anneau pour les trouver.', \n",
    "          'Un anneau pour les amener tous et dans les ténèbres les lier.']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7cabkp1w91Ei"
   },
   "source": [
    "## Tokenization\n",
    "\n",
    "La tokenization est le processus consistant à transformer une ou plusieurs chaînes de caractères continues en un ensemble d'unités distinctes. L'unité ainsi obtenue est appelée token. Grâce à la tokenization, il devient possible de traitrer indépendemment chaque unité d'un texte, facilitant ainsi leur encodage.\n",
    "\n",
    "Généralement, l'unité choisie est le mot (plus exactement l'unité lexicale), mais on peut également tokenizer à des niveaux plus fins, comme celui des sous-mots ou des caractères.\n",
    "\n",
    "\n",
    "```\n",
    "\"La batterie est arrivée cassée au niveau de la nappe de connexion.\"\n",
    "\n",
    "Mot : ['La', 'batterie', 'est', 'arrivée', 'cassée', 'au', 'niveau', 'de', 'la', 'nappe', 'de', 'connexion']\n",
    "\n",
    "Sous-mot : ['La', 'batterie', 'est', 'arriv', '##ée', 'cass', '##ée', 'au', 'niveau', 'de', 'la', 'nappe', 'de', 'connexion']\n",
    "\n",
    "Caractères : ['L', 'a', 'b', 'a', 't', 't', 'e', 'r', 'i', 'e', ...]\n",
    "\n",
    "```\n",
    "\n",
    "Tokenizer est une tâche loin d'être triviale: on ne peut en effet pas simplement diviser les phrases aux niveaux des espaces seulements. Il suffit de prendre la phrase ci-dessus pour s'en rendre compte:\n",
    "\n",
    "\n",
    "``Y-a-t-il de nouveaux arrivants?``\n",
    "\n",
    "Si l'on divise aux espaces simplements, on obtient ``['Y-a-t-il', 'de', 'nouveaux', 'arrivants?']``. On vient donc bien qu'il y-a au moins trois unités lexicales distinctes dans le premier token, et deux dans le dernier token. Ainsi, tokenizer demande de prendre en compte les différents signes graphiques et les particularités de chaque langues pour être efficace. \n",
    "\n",
    "On reviendra plus en détails sur la tokenization, qui est un sujet d'études à part entière en NLP. Dans l'immédiat et de manière générale, on peut se contenter de tokenizer au niveau des mots. De plus, des outils comme Spacy, OpenNLP ou torchtext proposent des tokenizer très performants. Ainsi, à moins de ne travailler sur une langue ou un problème bien particulier, il est recommandé d'utiliser les outils déjà existants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Au vu de la simplicité de notre corpus, nous nous contenterons de tokenizer au niveaux des espaces entre les mots, avec la fonction ci-dessous:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Tokenize en splittant le text au niveau des espaces. Retourne une liste.\n",
    "    \"\"\"\n",
    "    split_text = text.split()\n",
    "    \n",
    "    return split_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc : Un anneau pour les gouverner tous.\n",
      "Split doc : ['Un', 'anneau', 'pour', 'les', 'gouverner', 'tous.']\n",
      "Doc : Un anneau pour les trouver.\n",
      "Split doc : ['Un', 'anneau', 'pour', 'les', 'trouver.']\n",
      "Doc : Un anneau pour les amener tous et dans les ténèbres les lier.\n",
      "Split doc : ['Un', 'anneau', 'pour', 'les', 'amener', 'tous', 'et', 'dans', 'les', 'ténèbres', 'les', 'lier.']\n"
     ]
    }
   ],
   "source": [
    "tokenized_corpus = []\n",
    "for doc in corpus: # pour chaque document\n",
    "    print(\"Doc :\", doc)\n",
    "    split_doc = tokenize(doc) # on tokenize le document\n",
    "    print(\"Split doc :\", split_doc)\n",
    "    tokenized_corpus.append(split_doc) # on l'ajoute a la liste\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constituer le vocabulaire\n",
    "\n",
    "Un vocabulaire, ou lexique, est un ensemble fini de mots. Ce vocabulaire sera notre véritable sac de mot qui nous servira de référence pour encoder nos documents. \n",
    "\n",
    "Lorsqu'on utilise la méthode de sac de mots, chaque mot du vocabulaire devient une des features que l'on utilise pour entraîner le modèle. Ainsi, si l'on a 40 000 samples dans notre dataset, et que notre vocabulaire contient 10 000 mots, on a donc une matrice de ``(40 000 x 10 000)``.\n",
    "\n",
    "Pour créer ce vocabulaire en Python, il nous suffit d'utiliser le type set() et d'y ajouter chaque mot contenu dans nos documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulaire : {'ténèbres', 'dans', 'Un', 'amener', 'gouverner', 'lier.', 'tous', 'et', 'trouver.', 'pour', 'tous.', 'anneau', 'les'}\n",
      "Taille du vocabulaire : 13\n"
     ]
    }
   ],
   "source": [
    "vocabulary = set()\n",
    "for doc in tokenized_corpus:\n",
    "  vocabulary.update(doc)\n",
    "\n",
    "print(\"Vocabulaire :\", vocabulary)\n",
    "print(\"Taille du vocabulaire :\", len(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour simplifier, nous allons créer un DataFrame contenant ce corpus ainsi que ce vocabulaire:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encodage\n",
    "\n",
    "Dans le modèle de sac de mots, l'étape d'encodage va consister à associer une valeur numérique à chaque token contenu dans le document que l'on traite. Cette valeur numérique diffère selon la méthode d'encodage que l'on choisit. Ainsi pour chaque mot du vocabulaire, on peut:\n",
    "\n",
    "1. indiquer si celui-ci est présent (1) ou non (0) dans le document\n",
    "2. indiquer sa fréquence d'apparition, c'est-à-dire combien de fois celui-ci apparaît dans le document\n",
    "3. indiquer son poids par rapport aux documents et aux autre mots du vocabulaire (ce qui correspond à la méthode TF-IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1- Présence / Abscence du mot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulaire : {'ténèbres', 'dans', 'Un', 'amener', 'gouverner', 'lier.', 'tous', 'et', 'trouver.', 'pour', 'tous.', 'anneau', 'les'}\n",
      "Doc : Un anneau pour les gouverner tous.\n",
      "ténèbres : 0\n",
      "dans : 0\n",
      "Un : 1\n",
      "amener : 0\n",
      "gouverner : 1\n",
      "lier. : 0\n",
      "tous : 0\n",
      "et : 0\n",
      "trouver. : 0\n",
      "pour : 1\n",
      "tous. : 1\n",
      "anneau : 1\n",
      "les : 1\n",
      "\n",
      "---\n",
      "\n",
      "Doc : Un anneau pour les trouver.\n",
      "ténèbres : 0\n",
      "dans : 0\n",
      "Un : 1\n",
      "amener : 0\n",
      "gouverner : 0\n",
      "lier. : 0\n",
      "tous : 0\n",
      "et : 0\n",
      "trouver. : 1\n",
      "pour : 1\n",
      "tous. : 0\n",
      "anneau : 1\n",
      "les : 1\n",
      "\n",
      "---\n",
      "\n",
      "Doc : Un anneau pour les amener tous et dans les ténèbres les lier.\n",
      "ténèbres : 1\n",
      "dans : 1\n",
      "Un : 1\n",
      "amener : 1\n",
      "gouverner : 0\n",
      "lier. : 1\n",
      "tous : 1\n",
      "et : 1\n",
      "trouver. : 0\n",
      "pour : 1\n",
      "tous. : 0\n",
      "anneau : 1\n",
      "les : 1\n",
      "\n",
      "---\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ténèbres</th>\n",
       "      <th>dans</th>\n",
       "      <th>Un</th>\n",
       "      <th>amener</th>\n",
       "      <th>gouverner</th>\n",
       "      <th>lier.</th>\n",
       "      <th>tous</th>\n",
       "      <th>et</th>\n",
       "      <th>trouver.</th>\n",
       "      <th>pour</th>\n",
       "      <th>tous.</th>\n",
       "      <th>anneau</th>\n",
       "      <th>les</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Un anneau pour les gouverner tous.</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Un anneau pour les trouver.</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Un anneau pour les amener tous et dans les ténèbres les lier.</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    ténèbres  dans  Un  \\\n",
       "Un anneau pour les gouverner tous.                         0     0   1   \n",
       "Un anneau pour les trouver.                                0     0   1   \n",
       "Un anneau pour les amener tous et dans les ténè...         1     1   1   \n",
       "\n",
       "                                                    amener  gouverner  lier.  \\\n",
       "Un anneau pour les gouverner tous.                       0          1      0   \n",
       "Un anneau pour les trouver.                              0          0      0   \n",
       "Un anneau pour les amener tous et dans les ténè...       1          0      1   \n",
       "\n",
       "                                                    tous  et  trouver.  pour  \\\n",
       "Un anneau pour les gouverner tous.                     0   0         0     1   \n",
       "Un anneau pour les trouver.                            0   0         1     1   \n",
       "Un anneau pour les amener tous et dans les ténè...     1   1         0     1   \n",
       "\n",
       "                                                    tous.  anneau  les  \n",
       "Un anneau pour les gouverner tous.                      1       1    1  \n",
       "Un anneau pour les trouver.                             0       1    1  \n",
       "Un anneau pour les amener tous et dans les ténè...      0       1    1  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Vocabulaire :\", vocabulary)\n",
    "data = []\n",
    "for i, doc in enumerate(tokenized_corpus):\n",
    "    print(\"Doc :\", \" \".join(doc))\n",
    "    vocab_count = []\n",
    "    for x in vocabulary:\n",
    "        if x in doc:\n",
    "            print(f\"{x} : 1\")\n",
    "            vocab_count.append(1)\n",
    "        else:\n",
    "            print(f\"{x} : 0\")\n",
    "            vocab_count.append(0)\n",
    "    data.append(vocab_count)\n",
    "    print(\"\\n---\\n\")\n",
    "    \n",
    "binary_df = pd.DataFrame(data = data,\n",
    "                        columns = list(vocabulary))\n",
    "binary_df.index = corpus\n",
    "binary_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-Fréquence d'apparition des mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulaire : {'ténèbres', 'dans', 'Un', 'amener', 'gouverner', 'lier.', 'tous', 'et', 'trouver.', 'pour', 'tous.', 'anneau', 'les'}\n",
      "Doc : Un anneau pour les gouverner tous.\n",
      "ténèbres : 0\n",
      "dans : 0\n",
      "Un : 1\n",
      "amener : 0\n",
      "gouverner : 1\n",
      "lier. : 0\n",
      "tous : 0\n",
      "et : 0\n",
      "trouver. : 0\n",
      "pour : 1\n",
      "tous. : 1\n",
      "anneau : 1\n",
      "les : 1\n",
      "\n",
      "---\n",
      "\n",
      "Doc : Un anneau pour les trouver.\n",
      "ténèbres : 0\n",
      "dans : 0\n",
      "Un : 1\n",
      "amener : 0\n",
      "gouverner : 0\n",
      "lier. : 0\n",
      "tous : 0\n",
      "et : 0\n",
      "trouver. : 1\n",
      "pour : 1\n",
      "tous. : 0\n",
      "anneau : 1\n",
      "les : 1\n",
      "\n",
      "---\n",
      "\n",
      "Doc : Un anneau pour les amener tous et dans les ténèbres les lier.\n",
      "ténèbres : 1\n",
      "dans : 1\n",
      "Un : 1\n",
      "amener : 1\n",
      "gouverner : 0\n",
      "lier. : 1\n",
      "tous : 1\n",
      "et : 1\n",
      "trouver. : 0\n",
      "pour : 1\n",
      "tous. : 0\n",
      "anneau : 1\n",
      "les : 3\n",
      "\n",
      "---\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ténèbres</th>\n",
       "      <th>dans</th>\n",
       "      <th>Un</th>\n",
       "      <th>amener</th>\n",
       "      <th>gouverner</th>\n",
       "      <th>lier.</th>\n",
       "      <th>tous</th>\n",
       "      <th>et</th>\n",
       "      <th>trouver.</th>\n",
       "      <th>pour</th>\n",
       "      <th>tous.</th>\n",
       "      <th>anneau</th>\n",
       "      <th>les</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Un anneau pour les gouverner tous.</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Un anneau pour les trouver.</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Un anneau pour les amener tous et dans les ténèbres les lier.</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    ténèbres  dans  Un  \\\n",
       "Un anneau pour les gouverner tous.                         0     0   1   \n",
       "Un anneau pour les trouver.                                0     0   1   \n",
       "Un anneau pour les amener tous et dans les ténè...         1     1   1   \n",
       "\n",
       "                                                    amener  gouverner  lier.  \\\n",
       "Un anneau pour les gouverner tous.                       0          1      0   \n",
       "Un anneau pour les trouver.                              0          0      0   \n",
       "Un anneau pour les amener tous et dans les ténè...       1          0      1   \n",
       "\n",
       "                                                    tous  et  trouver.  pour  \\\n",
       "Un anneau pour les gouverner tous.                     0   0         0     1   \n",
       "Un anneau pour les trouver.                            0   0         1     1   \n",
       "Un anneau pour les amener tous et dans les ténè...     1   1         0     1   \n",
       "\n",
       "                                                    tous.  anneau  les  \n",
       "Un anneau pour les gouverner tous.                      1       1    1  \n",
       "Un anneau pour les trouver.                             0       1    1  \n",
       "Un anneau pour les amener tous et dans les ténè...      0       1    3  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Vocabulaire :\", vocabulary)\n",
    "data = []\n",
    "for i, doc in enumerate(tokenized_corpus):\n",
    "    print(\"Doc :\", \" \".join(doc))\n",
    "    vocab_count = []\n",
    "    for x in vocabulary:\n",
    "        if x in doc:\n",
    "            count = doc.count(x)\n",
    "            print(f\"{x} : {count}\")\n",
    "            vocab_count.append(count)\n",
    "        else:\n",
    "            print(f\"{x} : 0\")\n",
    "            vocab_count.append(0)\n",
    "    data.append(vocab_count)\n",
    "    print(\"\\n---\\n\")\n",
    "    \n",
    "freq_df = pd.DataFrame(data = data,\n",
    "                        columns = list(vocabulary))\n",
    "freq_df.index = corpus\n",
    "freq_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note\n",
    "\n",
    "En regardant le vocabulaire ci-dessus, on peut déjà voir les soucis que peut poser une simple tokenization en divisant le texte au niveau des espaces: Ici, on voit les mots ``[lier., trouver., tous.]`` suivis d'un point, alors qu'il ne devrait pas y être. \n",
    "\n",
    "De même, on voit le mot ``Un`` avec une majuscule: ainsi, si l'on avait eu le mot ``un`` sans majuscule, celui-ci aurait compté comme un autre token. Ceci montre l'importance de normaliser sont corpus avant de le tokenizer. Nous reviendrons plus tard sur les différentes méthodes de normalisation.\n",
    "\n",
    "Enfin, on peut voir que les mots les plus fréquents ou qui apparraissent systématiquement dans les trois documents sont des mots vides tels que ``[les, dans, Un, et, pour]``. Ces mots-vides, que l'on appelle également ``stopwords``, sont ceux qui apparaissent le plus fréquemment dans un corpus et donc par conséquent, sont ceux qui apportent le moins d'information (selon la loi de Zipf). Supprimer ces mots-vides du vocabulaire est généralement nécessaire afin d'en réduire la taille et ainsi conserver de l'espace mémoire. Nous reviendrons également sur cette notion plus tard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer\n",
    "\n",
    "Bien que l'on puisse implémenter nous-même les étapes précédentes, cela peut devenir laborieux si l'on répète l'opération régulièrement, sans compter si l'on traite des corpus bien plus grands que notre faux corpus. Ainsi, on peut utiliser l'objet ``CountVectorizer`` de ``scikit-learn`` pour réaliser les mêmes opérations et plus encore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme le ``LabelEncoder``, le ``CountVectorizer`` est un transformer: il possède une fonction ``fit()`` lui permettant de s'adapter aux données ainsi qu'une fonction ``transform()`` lui permettant de transformer ces données sous une autre forme. \n",
    "\n",
    "Ici, ``CountVectorizer()`` permet de transformer un corpus de texte en vecteurs d'occurrences de mots, comme nous l'avons fait précédemment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary :  ['amener', 'anneau', 'dans', 'et', 'gouverner', 'les', 'lier', 'pour', 'tous', 'trouver', 'ténèbres', 'un']\n",
      "Matrice d'occurrences :\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amener</th>\n",
       "      <th>anneau</th>\n",
       "      <th>dans</th>\n",
       "      <th>et</th>\n",
       "      <th>gouverner</th>\n",
       "      <th>les</th>\n",
       "      <th>lier</th>\n",
       "      <th>pour</th>\n",
       "      <th>tous</th>\n",
       "      <th>trouver</th>\n",
       "      <th>ténèbres</th>\n",
       "      <th>un</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Un anneau pour les gouverner tous.</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Un anneau pour les trouver.</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Un anneau pour les amener tous et dans les ténèbres les lier.</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    amener  anneau  dans  et  \\\n",
       "Un anneau pour les gouverner tous.                       0       1     0   0   \n",
       "Un anneau pour les trouver.                              0       1     0   0   \n",
       "Un anneau pour les amener tous et dans les ténè...       1       1     1   1   \n",
       "\n",
       "                                                    gouverner  les  lier  \\\n",
       "Un anneau pour les gouverner tous.                          1    1     0   \n",
       "Un anneau pour les trouver.                                 0    1     0   \n",
       "Un anneau pour les amener tous et dans les ténè...          0    3     1   \n",
       "\n",
       "                                                    pour  tous  trouver  \\\n",
       "Un anneau pour les gouverner tous.                     1     1        0   \n",
       "Un anneau pour les trouver.                            1     0        1   \n",
       "Un anneau pour les amener tous et dans les ténè...     1     1        0   \n",
       "\n",
       "                                                    ténèbres  un  \n",
       "Un anneau pour les gouverner tous.                         0   1  \n",
       "Un anneau pour les trouver.                                0   1  \n",
       "Un anneau pour les amener tous et dans les ténè...         1   1  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.fit(corpus)\n",
    "X = vectorizer.transform(corpus)\n",
    "\n",
    "# on peut reduire les deux dernieres lignes en une seule :\n",
    "# X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(\"Vocabulary : \", vectorizer.get_feature_names())\n",
    "print(\"Matrice d'occurrences :\")\n",
    "# print(X.toarray())\n",
    "\n",
    "vec_df = pd.DataFrame(data = X.toarray(),\n",
    "                        columns = vectorizer.get_feature_names())\n",
    "vec_df.index = corpus\n",
    "vec_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note\n",
    "\n",
    "Si l'on regarde le vocabulaire plus haut, on peut voir que cette fois, il n'y a pas de point aux mots ``[lier, trouver, tous]``, contrairement à ce que l'on a fait plus haut. De même, le mot ``Un`` apparaît ici en minuscule.\n",
    "\n",
    "``CountVectorizer`` se charge lui-même de la tokenization, pour laquelle il prend en charge certains aspects de prétraitement, comme la suppression des ponctuations ou le passage de l'intégrité du texte en minuscule. Nous verrons plus tard comment adapter ces paramètres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    A déconseiller - Article n'a fonctionné qu'une...\n",
       "1    Si vous voulez être déçu achetez le produit ! ...\n",
       "2    Écran de mauvaise qualité, car il s'use en peu...\n",
       "3    Cet engin ne sert à rien les sons sont pourris...\n",
       "4    Très beau produit mais la grue n'a pas fonctio...\n",
       "Name: texts, dtype: object"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant que nous savons comment encoder des données textuelles en vecteurs, nous pouvons appliquer cette méthode à l'ensemble du ``amazon-reviews`` (train, dev et test sets compris). Pour s'assurer que chaque dataset soit traité de la même façon, nous allons d'abord entraîner le ``CountVectorizer`` puis écrire une fonction permettant de transformer les données de la même manière:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "data = pd.concat([train['texts'], dev['texts'], test['texts']])\n",
    "# fit_transform regroupe les deux étapes fit et transform en une seule\n",
    "vectorizer.fit(data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_text(vectorizer, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forme de la matrice : (200000, 52706)\n",
      "Forme de la matrice : (5000, 52706)\n",
      "Forme de la matrice : (5000, 52706)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train, y_train = vectorizer.transform(train['texts']), train['classes']\n",
    "print(\"Forme de la matrice :\", X_train.shape)\n",
    "\n",
    "X_dev, y_dev = vectorizer.transform(dev['texts']), dev['classes']\n",
    "print(\"Forme de la matrice :\", X_dev.shape)\n",
    "\n",
    "X_test, y_test = vectorizer.transform(test['texts']), test['classes']\n",
    "print(\"Forme de la matrice :\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons maintenant utiliser ces données numériques de la même manière que les données que nous avions pour le ``iris-dataset``, et les donner en entrée à un algorithme tel que KNN our LogisticRegression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gutyh/anaconda3/envs/cours_dl/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4986"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(X_dev, y_dev)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
