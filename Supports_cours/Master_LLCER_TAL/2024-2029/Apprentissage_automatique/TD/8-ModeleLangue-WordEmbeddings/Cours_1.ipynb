{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modèle de langue et word embeddings\n",
    "\n",
    "Les modèles de langue sont des modèles probabilistes d'une langue. A partir d'un corpus d'étude, ils apprennent la probabilité d'apparition d'un terme, ainsi que la probabilité qu'un terme suive un autre terme ou séquence de termes. Ainsi, les modèles de langues appartiennent à la famille des méthodes non-supervisées. Ils sont particulièrement adapté pour étudier la structure d'une langue, mais aussi adaptés à des tâches comme la reconnaissance de langue, l'analyse en partie du discours ou la génération de texte. \n",
    "\n",
    "Les premiers modèles de language reposent sur des méthodes statistiques. Plus récémment, les principaux modèles de langues tels que BERT ou GPT reposent sur les méthodes neuronales. Si ces derniers sont bien plus puissants et efficaces, les ressources informatiques nécessaires pour les faire tourner sont bien plus importantes.  \n",
    "\n",
    "Dans ce cours, nous allons entraîner un modèle de langue statistique à l'aide de la librairie NLTK. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading regex-2023.8.8-cp39-cp39-macosx_10_9_x86_64.whl (294 kB)\n",
      "\u001b[K     |████████████████████████████████| 294 kB 6.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: joblib in /Users/nicolasgutehrle/opt/anaconda3/envs/cours/lib/python3.9/site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: tqdm in /Users/nicolasgutehrle/opt/anaconda3/envs/cours/lib/python3.9/site-packages (from nltk) (4.62.3)\n",
      "Requirement already satisfied: click in /Users/nicolasgutehrle/opt/anaconda3/envs/cours/lib/python3.9/site-packages (from nltk) (8.1.7)\n",
      "Installing collected packages: regex, nltk\n",
      "Successfully installed nltk-3.8.1 regex-2023.8.8\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modèle n-gramme\n",
    "\n",
    "Un modèle n-gramme détermine le mot suivant le plus probable à partir d'une séquence n-1 de mots. Par exemple, un modèle bigramme déterminera la probabilité qu'un mot apparaisse en fonction du mot précédent, un trigramme à partir des deux mots précédents, et ainsi de suite. Un modèle unigramme lui ne repose que sur la probabilité d'apparition du mot dans le corpus, sans prendre en compte ce qui précéde. \n",
    "\n",
    "Exemple : \n",
    "\n",
    "p(There was heavy rainfall) = p(START, There, was, heavy, rainfall, END) = p(There|START)p(was|There)p(heavy|There was)p(rainfall|There was heavy)p(END|There was heavy rainfall)\n",
    "\n",
    "Cependant, on ne peut réellement calculer la probabilité d'apparition d'une séquence aussi longue. On peut cependant calculer les probabilités d'apparition de chaque n-gramme précédents. Ainsi : \n",
    "\n",
    "p(There was heavy rainfall) = p(There|START)p(was|There)p(heavy|was)p(rainfall|heavy)p(END|rainfall)\n",
    "\n",
    "Pour la génération de texte, les modèles à partir de trigrammes sont les plus intéressants. Cependant, la taille du modèle dépend de la tâche et du type de données traité."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Préparer les données\n",
    "\n",
    "Pour entraîner un modèle, il nous faut des liste de documents tokénisé. Ci-dessous, la variable \"corpus\" contient deux \"phrases\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    ['a', 'b', 'c'],\n",
    "    ['a', 'c', 'd', 'c', 'e', 'f']\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK permet de rapidement constituer des n-grammes. Ci-dessous, on produit des bigrammes à partir de nos séquences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigrams\n",
      "[('a', 'b'), ('b', 'c')]\n",
      "[('a', 'c'), ('c', 'd'), ('d', 'c'), ('c', 'e'), ('e', 'f')]\n",
      "Trigrams\n",
      "[('a', 'b', 'c')]\n",
      "[('a', 'c', 'd'), ('c', 'd', 'c'), ('d', 'c', 'e'), ('c', 'e', 'f')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import bigrams, trigrams\n",
    "\n",
    "print('Bigrams')\n",
    "for x in corpus:\n",
    "    gram = list(bigrams(x))\n",
    "    print(gram)\n",
    "\n",
    "print('Trigrams')\n",
    "for x in corpus:\n",
    "    gram = list(trigrams(x))\n",
    "    print(gram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est important de distinguer l'occurrence d'un mot au sein de la phrase d'au début ou de la fin de phrase. Pour cela, on ajoute à la séquence deux caractères spéciaux, **s** et **/s** pour indiquer respectivement le début et la fin de phrase. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'a', 'b', 'c', '</s>']\n",
      "['<s>', 'a', 'c', 'd', 'c', 'e', 'f', '</s>']\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import pad_sequence\n",
    "\n",
    "for x in corpus:\n",
    "    # l'argument n précise que l'on travaille sur des bigrammes\n",
    "    seq = pad_sequence(x, n=2, pad_left=True, pad_right=True, left_pad_symbol='<s>', right_pad_symbol='</s>')\n",
    "    print(list(seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction \"pad_both_ends\" facilite l'emploi de \"pad_sequence\". Ci-dessous, une fonction pour prétraiter un corpus. Notez l'argument \"gram_func\" qui attend une fonction pour transformer la séquence en n-gramme (ex: fonction bigrams ou trigrams de NLTK). L'argument \"n\" doit correspondre au n-gramme donné."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'a', 'b', 'c', '</s>']\n",
      "['<s>', 'a', 'c', 'd', 'c', 'e', 'f', '</s>']\n"
     ]
    }
   ],
   "source": [
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "\n",
    "for x in corpus:\n",
    "    # l'argument n précise que l'on travaille sur des bigrammes\n",
    "    seq = pad_both_ends(x, n=2)\n",
    "    print(list(seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afin d'avoir un modèle plus robuste, on peut l'entraîner sur des bigrammes et unigrammes à la fois. NLTK met à disposition la fonction \"everygram\", qui génère tous les n-grammes d'une séquence jusqu'à n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<s>',), ('<s>', '<s>'), ('<s>', '<s>', 'a'), ('<s>',), ('<s>', 'a'), ('<s>', 'a', 'b'), ('a',), ('a', 'b'), ('a', 'b', 'c'), ('b',), ('b', 'c'), ('b', 'c', '</s>'), ('c',), ('c', '</s>'), ('c', '</s>', '</s>'), ('</s>',), ('</s>', '</s>'), ('</s>',)]\n",
      "[('<s>',), ('<s>', '<s>'), ('<s>', '<s>', 'a'), ('<s>',), ('<s>', 'a'), ('<s>', 'a', 'c'), ('a',), ('a', 'c'), ('a', 'c', 'd'), ('c',), ('c', 'd'), ('c', 'd', 'c'), ('d',), ('d', 'c'), ('d', 'c', 'e'), ('c',), ('c', 'e'), ('c', 'e', 'f'), ('e',), ('e', 'f'), ('e', 'f', '</s>'), ('f',), ('f', '</s>'), ('f', '</s>', '</s>'), ('</s>',), ('</s>', '</s>'), ('</s>',)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import everygrams\n",
    "n = 3\n",
    "for x in corpus:\n",
    "    # l'argument n précise que l'on travaille sur des bigrammes\n",
    "    padded_seq = pad_both_ends(x, n=n)\n",
    "    padded_grams = list(everygrams(padded_seq, max_len=n))\n",
    "    print(padded_grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le modèle nécessite également un vocabulaire, c'est à dire d'un ensemble de mots connus du modèle. Les symboles de début et fin de phrases font partie de ce vocabulaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', 'a', 'b', 'c', '</s>', '<s>', 'a', 'c', 'd', 'c', 'e', 'f', '</s>']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = [token for sent in text for token in pad_both_ends(sent, n=2)]\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction \"padded_everygram_pipeline\" nous permet de procéder à toutes ces étapes à l'aide d'une seule fonction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "n = 2\n",
    "data, vocab = padded_everygram_pipeline(n, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<s>',)\n",
      "('<s>', 'a')\n",
      "('a',)\n",
      "('a', 'b')\n",
      "('b',)\n",
      "('b', 'c')\n",
      "('c',)\n",
      "('c', '</s>')\n",
      "('</s>',)\n",
      "('<s>',)\n",
      "('<s>', 'a')\n",
      "('a',)\n",
      "('a', 'c')\n",
      "('c',)\n",
      "('c', 'd')\n",
      "('d',)\n",
      "('d', 'c')\n",
      "('c',)\n",
      "('c', 'e')\n",
      "('e',)\n",
      "('e', 'f')\n",
      "('f',)\n",
      "('f', '</s>')\n",
      "('</s>',)\n"
     ]
    }
   ],
   "source": [
    "for x in data:\n",
    "    for y in x:\n",
    "        print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>\n",
      "a\n",
      "b\n",
      "c\n",
      "</s>\n",
      "<s>\n",
      "a\n",
      "c\n",
      "d\n",
      "c\n",
      "e\n",
      "f\n",
      "</s>\n"
     ]
    }
   ],
   "source": [
    "for x in vocab:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entraîner le modèle\n",
    "\n",
    "Nous pouvons désormais entraîner un modèle de langue. Nous allons entraîner un modèle MLE (Maximum Likelihood Estimator), qui compte la fréquence de chaque n-gramme puis les normalise pour que ces valeurs soient contenues entre 0 et 1. Pour plus de détails sur le fonction, voir Jurafsky. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.lm import MLE\n",
    "n = 2\n",
    "lm = MLE(n)\n",
    "# le vocabulaire est vide au départ\n",
    "len(lm.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.fit(data, vocab)\n",
    "len(lm.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/g9/npfr2mks4118dkv7g8ckgccr0000gn/T/ipykernel_68605/1042022009.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('a', 'b', 'c')\n",
      "('<UNK>', '<UNK>', '<UNK>')\n"
     ]
    }
   ],
   "source": [
    "print(lm.vocab.lookup(corpus[0]))\n",
    "# les mots \"aliens\", \"from\", \"Mars\" ne sont pas contenus dans le vocabulaire. Ils sont donc associés à un token UNK\n",
    "print(lm.vocab.lookup([\"aliens\", \"from\", \"Mars\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compte de \"a\"  2\n",
      "0.15384615384615385\n",
      "0.5\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# \"a\" apparait 2 fois dans le corpus\n",
    "print('Compte de \"a\" ', lm.counts['a'])\n",
    "\n",
    "# la probabilité que \"a\" apparaîsse dans le corpus\n",
    "print(lm.score(\"a\"))\n",
    "\n",
    "# la probabilité que \"b\" apparaîsse en étant précédé de \"a\" == 1/2\n",
    "print(lm.score(\"b\", [\"a\"]))\n",
    "\n",
    "# la probabilité que \"d\" apparaîsse en étant précédé de \"a\" == aucune\n",
    "print(lm.score(\"d\", [\"a\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluer un modèle de langue\n",
    "\n",
    "Comme pour l'apprentissage supervisé, il nous faut des données de test pour évaluer un modèle. Cependant, le modèle de langue ne peut s'évaluer en terme de Précision, Rappel et F1, puisqu'il ne s'agit ni de classer, ni de retourner des informations. \n",
    "\n",
    "Pour évaluer un modèle de langue, on se sert de la mesure de **perplexité**, c'est à dire la probabilité ou la surprise que les données de test correspondent au modèle de langue. Elle repose sur la notion d'entropie, c'est-à-dire d'incertitude, dans la théorie de l'information. Plus la perplexité est basse (donc la probabilité est haute), et plus les données de test correspondent. Ainsi, pour évaluer un modèle de langue, il nous faut des extraits de langues qui sont représentatifs. \n",
    "\n",
    "La perplexité n'est pas la seule mesure pour évaluer un modèle de langue, mais s'en est une des principales. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.449489742783178\n",
      "inf\n",
      "inf\n"
     ]
    }
   ],
   "source": [
    "# ces données correspondent à notre modèle\n",
    "test = [('a', 'b'), ('c', 'd')]\n",
    "print(lm.perplexity(test))\n",
    "\n",
    "# ces données ne correspondent pas à notre modèle: le modèle est inf(iniment) surpris\n",
    "test = [('a', 'c'), ('b', 'd')]\n",
    "print(lm.perplexity(test))\n",
    "\n",
    "test = [('1', '2'), ('3', '4')]\n",
    "print(lm.perplexity(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Générer un texte\n",
    "\n",
    "La force majeure des modèles de langues est de pouvoir générer des séquences de tokens (mots, POS tags...). On peut laisser le modèle tout générer seul ou bien lui donner une séquence de départ. De plus, on peut intégrer des règles pour modifier le processus de génération de texte. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c', '</s>', '<s>', 'a', 'c']\n",
      "['a', 'b', 'c', '</s>', 'c']\n",
      "['c', '</s>', '<s>', 'a', 'c']\n"
     ]
    }
   ],
   "source": [
    "# génération simple\n",
    "print(lm.generate(5, random_seed=42))\n",
    "\n",
    "# génération en spécifiant le début de la séquence\n",
    "print(lm.generate(5, text_seed=['<s>'],random_seed=42))\n",
    "\n",
    "print(lm.generate(5, text_seed=['<s>', 'a'],random_seed=42))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoothing\n",
    "\n",
    "Pour le moment, notre modèle n'est capable d'assigner une probabilité à une séquence seulement s'il l'a vu dans le corpus d'étude. Cependant, un caractère peut être présent dans le vocabulaire, mais apparaître dans un nouveau contexte dans un autre corpus. Un caractère peut également ne pas apparaître du tout dans le vocabulaire du modèle. Dans un tel cas, ce caractère aura une probabilité nulle qui lui sera assignée. Pour éviter cela, il faut procéder au **smoothing**. Les principaux algorithmes de smoothing sont :\n",
    "\n",
    "* Laplace (add-one) smoothing\n",
    "* add-k smoothing\n",
    "* stupid backoff\n",
    "* Kneser-Ney smoothing\n",
    "\n",
    "Une autre possibilité est d'employer l'**interpolation** ou le **backoff**\n",
    "\n",
    "* backoff : si une séquence n'existe pas, on réduit cette séquence jusqu'à en trouver une connue du modèle\n",
    "* interpolation : on fait la somme de la probabilité associée à chaque n-gramme contenu dans la séquence (unigrame + bigramme + ... + n-gramme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm import Laplace\n",
    "n = 2\n",
    "\n",
    "data, vocab = padded_everygram_pipeline(n, corpus)\n",
    "\n",
    "lm = Laplace(n)\n",
    "lm.fit(data, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.744562646538029\n",
      "7.416198487095664\n",
      "9.000000000000002\n"
     ]
    }
   ],
   "source": [
    "# ces données correspondent à notre modèle\n",
    "test = [('a', 'b'), ('c', 'd')]\n",
    "print(lm.perplexity(test))\n",
    "\n",
    "# ces données ne correspondent pas à notre modèle: le modèle est inf(iniment) surpris\n",
    "test = [('a', 'c'), ('b', 'd')]\n",
    "print(lm.perplexity(test))\n",
    "\n",
    "test = [('1', '2'), ('3', '4')]\n",
    "print(lm.perplexity(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm import AbsoluteDiscountingInterpolated\n",
    "n = 2\n",
    "\n",
    "data, vocab = padded_everygram_pipeline(n, corpus)\n",
    "\n",
    "lm = AbsoluteDiscountingInterpolated(order=n)\n",
    "lm.fit(data, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.2300398978808\n",
      "7.62564998111037\n",
      "inf\n"
     ]
    }
   ],
   "source": [
    "# ces données correspondent à notre modèle\n",
    "test = [('a', 'b'), ('c', 'd')]\n",
    "print(lm.perplexity(test))\n",
    "\n",
    "# ces données ne correspondent pas à notre modèle: le modèle est inf(iniment) surpris\n",
    "test = [('a', 'c'), ('b', 'd')]\n",
    "print(lm.perplexity(test))\n",
    "\n",
    "test = [('1', '2'), ('3', '4')]\n",
    "print(lm.perplexity(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ressources : \n",
    "\n",
    "Pour plus de détails dans les calculs de probabilités et la construction mathématique des modèles de langue : \n",
    "* Documentation NLTK : https://www.nltk.org/api/nltk.lm.html\n",
    "* Jurafsky et al : https://web.stanford.edu/~jurafsky/slp3/3.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r1MRPhizGhSf"
   },
   "source": [
    "## Les inconvénients de la méthode sac de mots\n",
    "\n",
    "Dans les cours précédents, nous avons utilisé la méthode du sac de mots afin d'encoder des données textuelles afin de pouvoir les donner en entrée à nos algorithmes. Cette méthode à été une des premières utilisée dans le domaine du TAL et du Machine Learning. En encodant chaque token par leur fréquence d'apparition ou par leur poids par rapport aux documents du corpus (Tf-idf), cette méthode permet de constituer des vecteurs de mots, qui sont ensuite utilisés comme données d'entraînement à nos modèles.\n",
    "\n",
    "Cependant, comme nous l'avons déjà vu, cette méthode d'encodage du texte et donc cette méthode de production des vecteurs posent plusieurs problèmes du point de vue informatique et linguistique:\n",
    "\n",
    "* Pour chaque samples de notre dataset, on créé un vecteur dont le nombre de dimensions est égal à la taille de notre vocabulaire. Ces vecteurs sont donc généralement composés de plusieurs dizaines de millier de dimensions, ce qui est très coûteux en terme de calcul.\n",
    "\n",
    "* De plus, pour chaque sample, seuls une très petite portion de notre vocabulaire y est présent. Ainsi, peut importe la méthode d'encodage que l'on choisit, le vecteur de mots sera essentiellement composé de zéros. L'essentielle des dimensions de notre vecteur ne sont donc pas pertinentes pour la compréhension du sample traité.\n",
    "\n",
    "* Le sac de mot ne conserve aucune information sur la structure du texte ni la syntaxe de la phrase. Or, un même vocabulaire peut prendre un sens très différent selon l'ordre dans lequel il est utilisé. \n",
    "\n",
    "* La valeur que prend un mot dépend de son encodage, c'est-à-dire soit de son nombre d'occurrences dans le corpus, soit de son poids par rapport aux autres termes dans le document et dans le corpus. Ainsi, il n'a pas de vraie valeur sémantique.\n",
    "\n",
    "Ainsi il est évident que la méthode sac de mots n'est pas suffisante si l'on souhaite améliorer les modèles existants ou réduire la puissance de calcul demandée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_1 = [0, 0, 10, ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embedings\n",
    "\n",
    "Les Words Embeddings sont très différents des sac de mots: Un Word Embeddings peut se comparer à un dictionnaire pour lequel chaque clé est un mot et chaque valeur est un vecteur représentant ce mot. La taille de ces vecteurs est de l'ordre de la centaine, et sont généralement constitués de 50, 100, 300 ou 500 dimensions, ce qui est extrêmement réduit par rapport au vecteur de vocabulaire du sac de mots. \n",
    "\n",
    "Ci-dessous, un exemple de faux Word Embbedings:\n",
    "\n",
    "``\n",
    "we = {\n",
    "    \"chat\": [0.33, 0.5, 0.86, 0.77, 0.23],\n",
    "    \"le\" : [0.21, 1, 0.93, 0.54, 0.64],\n",
    "    \"rat\" : [0.96, 0.16, 0.98, 0.88, 0.26],\n",
    "    \"mange\" : [0.63, 0.97, 0.4, 0.32, 0.43]\n",
    "}\n",
    "``\n",
    "\n",
    "En utilisant la méthode de sac de mots, on encoderait la phrase ``Le chat mange le rat`` avec un vecteur de la taille du vocabulaire et dans lequel on indiquerait la présence (one-hot encoding) oule nombre d'occurrences de chaque mot. A l'inverse en encodant cette phrases avec des Word Embeddings, chaque mot serait représenté par le vecteur qui lui est associé:\n",
    "\n",
    "``[\n",
    "    [0.21, 1, 0.93, 0.54, 0.64], ==> le\n",
    "    [0.33, 0.5, 0.86, 0.77, 0.23], ==> chat\n",
    "    [0.63, 0.97, 0.4, 0.32, 0.43], ==> mange\n",
    "    [0.21, 1, 0.93, 0.54, 0.64], ==> le\n",
    "    [0.96, 0.16, 0.98, 0.88, 0.26] ==> rat\n",
    "]``\n",
    "\n",
    "### Note\n",
    "\n",
    "Le terme Word Embedding fait référence au fait de représenter un mot sous forme de vecteur. Ainsi, un vecteur dans lequel on indique la fréquence d'apparition d'un mot dans un contexte donné ou bien son poids par rapport au corpus est un Word Embedding. Cependant, on distingue ces méthodes de celles où ces vecteurs sont appris de manière automatique et qui comporte une plus grande information sémantique: ce sont ces méthodes (Word2Vec, GloVe, fastText, ...) que l'on englobe communément dans le terme \"Word Embeddings\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "\n",
    "Le terme Word Embeddings est réellement devenu proéminent dans le domaine du TAL et du Machine Learning grâce l'algorithme ``Word2Vec``, développé par Mikolov et al (2013). A son apparition, Word2Vec a totalement chamboulé le domaine du TAL: en représentant les mots dans un espace vectorielle, il a permis de traiter l'aspect sémantique d'un vocabulaire. Ainsi, des performances jamais obtenues jusque là ont pu être atteinte, tandis que de nouveaux traitements linguistiques s'ouvraient.\n",
    "\n",
    "<img src='data/img/wordemb.png'>\n",
    "\n",
    "\n",
    "Ref: https://medium.com/@hari4om/word-embedding-d816f643140\n",
    "\n",
    "<img src='data/img/wordemb2.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sémantique, logique et mathématique\n",
    "\n",
    "Un intérêt majeur de représenter le vocabulaire sous forme de vecteur est que cela permet de réaliser des opérations logiques au travers d'opérations mathématiques. \n",
    "\n",
    "### Calcul de similarité\n",
    "\n",
    "Tout d'abord, cela permet pour un ou plusieurs mots donnés d'en trouver les plus similaires. Ci-dessous on peut voir que les mots \"cat\" et \"kitten\" sont très proches. Mathématiquement, on peut calculer la similarité entre ces deux vecteurs (donc deux mots) en calculant leur similarité cosinus.\n",
    "\n",
    "<img src='data/img/cat_kitten.png'>\n",
    "\n",
    "### Analogie\n",
    "\n",
    "Un autre aspect très important des Word Embeddings est qu'il permet de traiter l'analogie entre différents termes. Ci-dessous une analogie très célèbre dans le contexte des Word Embeddings:\n",
    "\n",
    "``Le mot 'homme' est au mot 'femme' ce que le mot 'roi' est au mot 'reine``\n",
    "\n",
    "Comme on peut le voir sur l'image ci-dessous, la relation entre les points (man - woman) et (king - queen) sont similaires et situés relativement proches les uns des autres.\n",
    "\n",
    "<img src='data/img/king_queen.png'>\n",
    "\n",
    "\n",
    "Grâce aux vecteurs, on peut exprimer l'analogie ci-dessus de manière mathématique avec des additions et des soustractions:\n",
    "\n",
    "`` (roi - homme) + femme = reine``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CBOW et SkipGram\n",
    "\n",
    "Contrairement aux travaux précédents, qui se basaient sur des matrices de co-occurrences de mots pour générer des vecteurs, Word2Vec consiste à entraîner un réseau de neurones à partir duquel on génère les vecteurs de mots. Le nombre de dimensions des vecteurs produits par cette méthode est de lors d'une à plusieurs centaines, ce qui est considérablement moins que les dizaines de millier de dimensions avec le sac de mots.\n",
    "\n",
    "``Word2Vec`` se divise en deux méthodes opposées d'apprentissage: ``CBOW (Continuous Bag of Words)`` et ``SkipGram``. \n",
    "\n",
    "<img src=\"data/img/cbow_skipgram.png\">\n",
    "\n",
    "\n",
    "* CBOW: le modèle doit prédire un mot en ayant comme donnée d'entrée son contexte direct, qui est constitué d'une fenêtre plus ou moins grande de mots\n",
    "\n",
    "<img src='data/img/bow.gif'>\n",
    "\n",
    "* Skip-gram: Le modèle prend en entrée un mot et doit prédire son contexte. C'est l'opposé de CBOW.\n",
    "\n",
    "<img src='data/img/skip.gif'>\n",
    "\n",
    "\n",
    "Lors de l'entraînement, le modèle apprend des paramètres de manière itérative qui lui permette de prédire (correctement ou non) les mots cibles. Ce sont ces paramètres qui vont constituer les vecteus de mots, et ainsi, ce sont ces paramètres et pas tant le modèle en lui-même que l'on va conserver.\n",
    "\n",
    "### Avantages et inconvénients de chaque méthode\n",
    "\n",
    "* Skip-gram est plus adapté aux petits corpus. De plus, elle représente mieux les mots relativement peu fréquents dans le corpus. Cependant, son temps d'entraînement est très long.\n",
    "* CBOW est plus adapté aux grands corpus. Il généralise mieux que Skip-gram, et représente donc mieux les mots plutôts fréquents. De plus, il est relativement rapide à entraîner. Cependant, il peut donner de mauvais résultats sur un corpus trop petit et les poids obtenus ne seront pas autant spécialisés qu'avec Skip-gram.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entraîner votre propre modèle\n",
    "\n",
    "### Charger les données\n",
    "\n",
    "Nous allons entraîner notre premier modèle sur la portion d'analyse de sentiments du ``amazon_reviews`` que nous avons sauvegardé précédemment. Cette portion se trouve dans le dossier ``data/multiclass`` de ce notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    A déconseiller - Article n'a fonctionné qu'une...\n",
       "1    Si vous voulez être déçu achetez le produit ! ...\n",
       "2    Écran de mauvaise qualité, car il s'use en peu...\n",
       "3    Cet engin ne sert à rien les sons sont pourris...\n",
       "4    Très beau produit mais la grue n'a pas fonctio...\n",
       "Name: texts, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('data/multiclass/as_train.csv')\n",
    "corpus = df.texts\n",
    "corpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim attend que les données d'entrées soit au minimum tokenizées. Pour l'exemple ci-dessous, nous nous contenterons de diviser les phrases au niveau des espaces, mais il est recommandé de prétraiter vos données de la même manière que précédemment, c'est à dire d'utiliser un vrai tokenizer, de supprimer les mots-vides, de transformer le texte en minuscule, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/nicolasgutehrle/opt/anaconda3/envs/cours/lib/python3.9/runpy.py\", line 188, in _run_module_as_main\n",
      "    mod_name, mod_spec, code = _get_module_details(mod_name, _Error)\n",
      "  File \"/Users/nicolasgutehrle/opt/anaconda3/envs/cours/lib/python3.9/runpy.py\", line 147, in _get_module_details\n",
      "    return _get_module_details(pkg_main_name, error)\n",
      "  File \"/Users/nicolasgutehrle/opt/anaconda3/envs/cours/lib/python3.9/runpy.py\", line 111, in _get_module_details\n",
      "    __import__(pkg_name)\n",
      "  File \"/Users/nicolasgutehrle/opt/anaconda3/envs/cours/lib/python3.9/site-packages/spacy/__init__.py\", line 15, in <module>\n",
      "    from .cli.info import info  # noqa: F401\n",
      "  File \"/Users/nicolasgutehrle/opt/anaconda3/envs/cours/lib/python3.9/site-packages/spacy/cli/__init__.py\", line 3, in <module>\n",
      "    from ._util import app, setup_cli  # noqa: F401\n",
      "  File \"/Users/nicolasgutehrle/opt/anaconda3/envs/cours/lib/python3.9/site-packages/spacy/cli/_util.py\", line 9, in <module>\n",
      "    import typer\n",
      "  File \"/Users/nicolasgutehrle/opt/anaconda3/envs/cours/lib/python3.9/site-packages/typer/__init__.py\", line 12, in <module>\n",
      "    from click.termui import get_terminal_size as get_terminal_size\n",
      "ImportError: cannot import name 'get_terminal_size' from 'click.termui' (/Users/nicolasgutehrle/opt/anaconda3/envs/cours/lib/python3.9/site-packages/click/termui.py)\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download fr_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'get_terminal_size' from 'click.termui' (/Users/nicolasgutehrle/opt/anaconda3/envs/cours/lib/python3.9/site-packages/click/termui.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/g9/npfr2mks4118dkv7g8ckgccr0000gn/T/ipykernel_68605/3207692533.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fr_core_news_sm'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/cours/lib/python3.9/site-packages/spacy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpipeline\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcli\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minfo\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mglossary\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexplain\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mabout\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/cours/lib/python3.9/site-packages/spacy/cli/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwasabi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_util\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msetup_cli\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# These are the actual functions, NOT the wrapped CLI commands. The CLI commands\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/cours/lib/python3.9/site-packages/spacy/cli/_util.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msrsly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhashlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtyper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mclick\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNoSuchOption\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mclick\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msplit_arg_string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/cours/lib/python3.9/site-packages/typer/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mclick\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtermui\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mecho_via_pager\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mecho_via_pager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mclick\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtermui\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0medit\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0medit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mclick\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtermui\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_terminal_size\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mget_terminal_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mclick\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtermui\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgetchar\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgetchar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mclick\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtermui\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlaunch\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlaunch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'get_terminal_size' from 'click.termui' (/Users/nicolasgutehrle/opt/anaconda3/envs/cours/lib/python3.9/site-packages/click/termui.py)"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('fr_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = nlp.pipe(corpus[:5000])\n",
    "train = [] # creer nouvelle liste qui servir de corpus tokenizé\n",
    "for doc in docs:\n",
    "    filtered_doc = [] # doc tokenizé\n",
    "    for token in doc:\n",
    "        if token.is_stop:\n",
    "            pass\n",
    "        elif token.is_punct:\n",
    "            pass\n",
    "        else:\n",
    "            filtered_doc.append(token.lemma_) # si token est bon, on l'ajoute a la liste\n",
    "            \n",
    "    train.append(filtered_doc) # on ajoute le doc tokenizé au nouveau corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim\n",
    "\n",
    "``scikit-learn`` ne permet pas d'utiliser ou d'entraîner des Words Embeddings. Pour cela, nous allons utiliser ``gensim``, une librairie de TAL spécialisée dans les tâches non-supervisées (topic modelling, word embeddings, ...). \n",
    "\n",
    "Elle propose également une implémentation de Word2Vec ainsi que d'autres algorithmes similaires tels que GloVe et FastText (que nous présenterons plus tard). Elle sert également d'API permettant d'utiliser ces trois algorithmes de la même manière."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec # on importe la classe Word2Vec\n",
    "# crééer le modèle et lui donner les données lance l'entraînement\n",
    "model = Word2Vec(sentences = train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Par défaut, le modèle Word2Vec de Gensim créé des vecteurs à 100 dimensions et utilise la méthode CBOW. De plus, il ne considère par défaut que les mots occurrants au moins 5 fois dans le corpus et utilie une fenêtre de 5 mots de contexte lors de l'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "The vocab attribute was removed from KeyedVector in Gensim 4.0.0.\nUse KeyedVector's .key_to_index dict, .index_to_key list, and methods .get_vecattr(key, attr) and .set_vecattr(key, attr, new_val) instead.\nSee https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/g9/npfr2mks4118dkv7g8ckgccr0000gn/T/ipykernel_12603/4247305146.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/cours/lib/python3.9/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mvocab\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    659\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 661\u001b[0;31m         raise AttributeError(\n\u001b[0m\u001b[1;32m    662\u001b[0m             \u001b[0;34m\"The vocab attribute was removed from KeyedVector in Gensim 4.0.0.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m             \u001b[0;34m\"Use KeyedVector's .key_to_index dict, .index_to_key list, and methods \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: The vocab attribute was removed from KeyedVector in Gensim 4.0.0.\nUse KeyedVector's .key_to_index dict, .index_to_key list, and methods .get_vecattr(key, attr) and .set_vecattr(key, attr, new_val) instead.\nSee https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4"
     ]
    }
   ],
   "source": [
    "model.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "The vocab attribute was removed from KeyedVector in Gensim 4.0.0.\nUse KeyedVector's .key_to_index dict, .index_to_key list, and methods .get_vecattr(key, attr) and .set_vecattr(key, attr, new_val) instead.\nSee https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/g9/npfr2mks4118dkv7g8ckgccr0000gn/T/ipykernel_12603/3286036687.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/cours/lib/python3.9/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mvocab\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    659\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 661\u001b[0;31m         raise AttributeError(\n\u001b[0m\u001b[1;32m    662\u001b[0m             \u001b[0;34m\"The vocab attribute was removed from KeyedVector in Gensim 4.0.0.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m             \u001b[0;34m\"Use KeyedVector's .key_to_index dict, .index_to_key list, and methods \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: The vocab attribute was removed from KeyedVector in Gensim 4.0.0.\nUse KeyedVector's .key_to_index dict, .index_to_key list, and methods .get_vecattr(key, attr) and .set_vecattr(key, attr, new_val) instead.\nSee https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4"
     ]
    }
   ],
   "source": [
    "for i, word in enumerate(model.wv.vocab):\n",
    "    if i == 10:\n",
    "        break\n",
    "    print(i, word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1766"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans Gensim, les vecteurs sont contenus dans la propriété ``wv`` de votre modèle. Pour obtenir l vecteur d'un mot, il suffit de le donner en index, comme pour un dictionnaire:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.25532645,  0.40520772,  0.08530292,  0.08051319,  0.1792276 ,\n",
       "       -0.6952582 ,  0.16616455,  0.8068265 , -0.32048878, -0.246494  ,\n",
       "       -0.24522711, -0.6452503 , -0.07073019,  0.12570128,  0.03112432,\n",
       "       -0.3486686 , -0.04763675, -0.47627515, -0.07456362, -0.6311179 ,\n",
       "        0.17140576,  0.27084276,  0.15460801, -0.25665915, -0.11724535,\n",
       "       -0.05270885, -0.38669258, -0.36303726, -0.33437285, -0.08058538,\n",
       "        0.30668992,  0.0790379 ,  0.12327613, -0.09210966, -0.27978987,\n",
       "        0.46599925,  0.01484592, -0.49812692, -0.20823085, -0.8380023 ,\n",
       "        0.17366855, -0.2844147 ,  0.02119729, -0.01766261,  0.27059153,\n",
       "       -0.11366633, -0.22479628, -0.11822856,  0.2405547 ,  0.16269894,\n",
       "        0.18236665, -0.33445388, -0.11590739, -0.01788492, -0.00612189,\n",
       "        0.35585123,  0.29613703,  0.04808021, -0.44746116,  0.11884031,\n",
       "        0.11987   ,  0.12157893, -0.00322383,  0.02806825, -0.51337767,\n",
       "        0.26281565,  0.11444645,  0.2360296 , -0.4522114 ,  0.5233524 ,\n",
       "       -0.30813748,  0.18072827,  0.41297647, -0.13555548,  0.28287637,\n",
       "        0.1566837 , -0.09109749, -0.11162833, -0.4227386 ,  0.19926196,\n",
       "       -0.21484539,  0.08470338, -0.531221  ,  0.60838515, -0.0721352 ,\n",
       "        0.05202443,  0.11786921,  0.58084244,  0.66696376,  0.12217856,\n",
       "        0.5011943 ,  0.30054766,  0.02920026,  0.17155269,  0.6554638 ,\n",
       "        0.28011742,  0.33611935, -0.4562862 ,  0.13897625, -0.3210282 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['article']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['article'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trouver le mot le plus similaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('achat', 0.9996308088302612),\n",
       " ('tenir', 0.9996005296707153),\n",
       " ('acheter', 0.9995850324630737),\n",
       " ('déçu', 0.9995845556259155),\n",
       " ('bref', 0.9995772838592529),\n",
       " ('inutilisable', 0.9995712041854858),\n",
       " ('niveau', 0.9995613694190979),\n",
       " ('bon', 0.9995589852333069),\n",
       " ('fonctionne', 0.9995576739311218),\n",
       " ('plastique', 0.999556303024292)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('recommande')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculer la similarité entre deux mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance 'article' - 'acheter' : 0.0017984509468078613\n",
      "Distance 'plastique' - 'bref' : 0.00031310319900512695\n"
     ]
    }
   ],
   "source": [
    "print(\"Distance 'article' - 'acheter' :\", model.wv.distance('article', 'acheter'))\n",
    "print(\"Distance 'plastique' - 'bref' :\", model.wv.distance('plastique', 'bref'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculer la similarité entre deux vecteurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance 'article' - 'produit' : [0.9998901]\n",
      "Distance 'article' - 'canard' : [0.9998109]\n"
     ]
    }
   ],
   "source": [
    "article = model.wv['article']\n",
    "produit = model.wv['produit']\n",
    "bref = model.wv['bref']\n",
    "\n",
    "print(\"Distance 'article' - 'produit' :\", model.wv.cosine_similarities(article, [produit]))\n",
    "print(\"Distance 'article' - 'canard' :\", model.wv.cosine_similarities(article, [bref]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifier les mots les plus similaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Amazon', 0.9999183416366577),\n",
       " ('produit', 0.9998900294303894),\n",
       " ('devoir', 0.9998878240585327),\n",
       " ('faire', 0.9998875260353088),\n",
       " ('amazon', 0.9998860359191895),\n",
       " ('livre', 0.9998849630355835),\n",
       " ('vendeur', 0.9998847246170044),\n",
       " ('renvoyer', 0.9998829364776611),\n",
       " ('commande', 0.9998828172683716),\n",
       " ('rembourser', 0.9998816847801208)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# identifie les mots les plus similaire d'un mot\n",
    "model.wv.similar_by_word('article')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Amazon'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# identifie le mot le plus similaires de la liste par rapport\n",
    "# a un autre mot\n",
    "list_word = ['produit', 'Amazon', 'bref']\n",
    "model.wv.most_similar_to_given('article', list_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('appareil', 0.9997512102127075),\n",
       " ('servir', 0.9997507333755493),\n",
       " ('petit', 0.9997495412826538),\n",
       " ('inutilisable', 0.9997475743293762),\n",
       " ('fonctionner', 0.9997460246086121),\n",
       " ('décevant', 0.9997374415397644),\n",
       " ('y', 0.9997371435165405),\n",
       " ('acheter', 0.9997363090515137),\n",
       " ('chose', 0.999734103679657),\n",
       " ('film', 0.9997307062149048)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# identifie les mots les plus similaires en se basant sur deux listes:\n",
    "# une liste positive et une liste négative\n",
    "# retourne les mots les plus similaires à un champ lexical donné par la liste positive\n",
    "# en faisant en sorte de s'éloigner du champ lexical des mots négatifs\n",
    "pos = ['objet', 'produit']\n",
    "neg = ['Amazon']\n",
    "model.wv.most_similar(pos, neg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifier un intrus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'objet'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_word = ['article', 'bref', 'produit', 'objet']\n",
    "model.wv.doesnt_match(list_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifier un mot plus proche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['produit',\n",
       " 'déçu',\n",
       " 'acheter',\n",
       " 'colis',\n",
       " '2',\n",
       " 'jour',\n",
       " 'faire',\n",
       " 'vendeur',\n",
       " 'y',\n",
       " 'mettre',\n",
       " 'fois',\n",
       " 'impossible',\n",
       " 'arriver',\n",
       " 'commander',\n",
       " 'vraiment',\n",
       " 'bon',\n",
       " 'commande',\n",
       " 'petit',\n",
       " 'prix',\n",
       " 'achat',\n",
       " 'recommande',\n",
       " 'livrer',\n",
       " '3',\n",
       " 'livraison',\n",
       " 'l',\n",
       " 'Amazon',\n",
       " 'remboursement',\n",
       " 'dommage',\n",
       " 'n',\n",
       " 'devoir',\n",
       " 'vouloir',\n",
       " 'j',\n",
       " 'utiliser',\n",
       " 'fonctionner',\n",
       " 'photo',\n",
       " 'problème',\n",
       " 'renvoyer',\n",
       " 'décevoir',\n",
       " 'voir',\n",
       " 'semaine',\n",
       " 'mal',\n",
       " 'téléphone',\n",
       " 'd',\n",
       " 'falloir',\n",
       " 'rembourser',\n",
       " 'attendre',\n",
       " 'Bonjour',\n",
       " 'temps',\n",
       " 'cassé',\n",
       " 'euro',\n",
       " 'livre',\n",
       " 'verre',\n",
       " '4',\n",
       " 'servir',\n",
       " 'plastique',\n",
       " 'savoir',\n",
       " 'emballage',\n",
       " 'appareil',\n",
       " 'coup',\n",
       " 'demande',\n",
       " 'c',\n",
       " 'éviter',\n",
       " 'bref',\n",
       " 'prendre',\n",
       " 'correspondre',\n",
       " 'réponse',\n",
       " 'avoir',\n",
       " 'passer',\n",
       " 'inutilisable',\n",
       " 'taille',\n",
       " 'pouvoir',\n",
       " '10',\n",
       " 'amazon',\n",
       " 'envoyer',\n",
       " 'commentaire',\n",
       " 'niveau',\n",
       " 'carton',\n",
       " 'grand',\n",
       " 'Article',\n",
       " 'couleur',\n",
       " '5',\n",
       " 'indiquer',\n",
       " 'marche',\n",
       " 'retourner',\n",
       " 'pourtant',\n",
       " 'part',\n",
       " 'demander',\n",
       " 'gros',\n",
       " 'venir',\n",
       " 'heure',\n",
       " 'cadeau',\n",
       " 'fin',\n",
       " 'perdre',\n",
       " 'poubelle',\n",
       " 'service',\n",
       " 'payer',\n",
       " 'peine',\n",
       " 'dire',\n",
       " 'essayer',\n",
       " 'contacter',\n",
       " 'site',\n",
       " 'marque',\n",
       " 'trouver',\n",
       " 'description',\n",
       " 'eau',\n",
       " 'pièce',\n",
       " 'conforme',\n",
       " 'inutile',\n",
       " 'complètement',\n",
       " 'être',\n",
       " 'attention',\n",
       " 'chose',\n",
       " 'donner',\n",
       " 'côté',\n",
       " 'livreur',\n",
       " 'état',\n",
       " 'réception',\n",
       " 'rapidement',\n",
       " 'attente',\n",
       " 'mail',\n",
       " 'prévoir',\n",
       " 'argent',\n",
       " 'cas',\n",
       " 'résultat',\n",
       " 'notice',\n",
       " 'malheureusement',\n",
       " 'enfant',\n",
       " 'prise',\n",
       " '6',\n",
       " 'vite',\n",
       " 'recu',\n",
       " 'arrive',\n",
       " 'message',\n",
       " 'arnaque',\n",
       " 'trouve',\n",
       " 'm',\n",
       " 'montre',\n",
       " 'offrir',\n",
       " 'sac',\n",
       " 'avis',\n",
       " 'déception',\n",
       " 'défectueux',\n",
       " 'installer',\n",
       " 'ouvrir',\n",
       " 'répondre',\n",
       " 'penser',\n",
       " 'difficile',\n",
       " 'obliger',\n",
       " 'manqu',\n",
       " 'point',\n",
       " 'partie',\n",
       " 'air',\n",
       " 'main',\n",
       " 'produire',\n",
       " 'changer',\n",
       " 'jeter',\n",
       " 'fils',\n",
       " 'finir',\n",
       " 'client',\n",
       " 'aller',\n",
       " 'carte',\n",
       " '15',\n",
       " 'modèle',\n",
       " 'suivi',\n",
       " 'qu',\n",
       " 'fuir',\n",
       " 's',\n",
       " 'long',\n",
       " 'inadmissible',\n",
       " 'compte',\n",
       " 'suite',\n",
       " 'noir',\n",
       " 'contact',\n",
       " 'étoile',\n",
       " 'livré',\n",
       " 'cordialement',\n",
       " 'décevant',\n",
       " 'écrire',\n",
       " 'réclamation',\n",
       " 'vendre',\n",
       " 'recommander',\n",
       " 'jeu',\n",
       " 'minute',\n",
       " 'film',\n",
       " 'correctement',\n",
       " 'vide',\n",
       " 'date',\n",
       " 'normal',\n",
       " 'pied',\n",
       " 'intérieur',\n",
       " 'tomber',\n",
       " 'moyen',\n",
       " 'porte',\n",
       " 'vitre',\n",
       " 'partir',\n",
       " 'odeur',\n",
       " 'vis',\n",
       " 'français',\n",
       " 'poste',\n",
       " 'moment',\n",
       " 'papier',\n",
       " 'blanc',\n",
       " 'frais',\n",
       " 'marquer',\n",
       " 'comprendre',\n",
       " 'manque',\n",
       " '8',\n",
       " 'erreur',\n",
       " 'racheter',\n",
       " 'lieu',\n",
       " 'marcher',\n",
       " 'laisser',\n",
       " 'garantie',\n",
       " 'cartouche',\n",
       " '20',\n",
       " 'heureusement',\n",
       " 'fournisseur',\n",
       " 'bouton',\n",
       " 'laisse',\n",
       " 'compatible',\n",
       " '7',\n",
       " 'premier',\n",
       " 'retirer',\n",
       " 'honte',\n",
       " 'espérer',\n",
       " 'faux',\n",
       " 'transporteur',\n",
       " 'commencer',\n",
       " 'ca',\n",
       " 'entendre',\n",
       " 'abîmer',\n",
       " 'fort',\n",
       " 'franchement',\n",
       " 'couper',\n",
       " 'cm',\n",
       " 'délai',\n",
       " 'effectuer',\n",
       " 'besoin',\n",
       " 'simple',\n",
       " 'fonction',\n",
       " 'sortir',\n",
       " '30',\n",
       " 'ouverture',\n",
       " 'envoi',\n",
       " 'poser',\n",
       " 'command',\n",
       " 'nouvelle',\n",
       " 'amazone',\n",
       " 'facile',\n",
       " 'descriptif',\n",
       " 'nouveau',\n",
       " 'positif',\n",
       " 'usb',\n",
       " 'bruit',\n",
       " 'clé',\n",
       " 'donne',\n",
       " 'vrai',\n",
       " 'page',\n",
       " 'reconnaître',\n",
       " 'finalement',\n",
       " 'moitié',\n",
       " 'décembre',\n",
       " 'portable',\n",
       " 'surprise',\n",
       " '2018',\n",
       " 'chien',\n",
       " 'fermeture',\n",
       " 'agir',\n",
       " 'rentrer',\n",
       " 'tapis',\n",
       " 'rester',\n",
       " 'annoncer',\n",
       " '2019',\n",
       " 'moteur',\n",
       " 'obtenir',\n",
       " 'adaptateur',\n",
       " 'permettre',\n",
       " 'remplacement']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.words_closer_than('article', 'objet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sauvegarder le modèle sur le disque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "savepath = 'data/w2v_model'\n",
    "if not os.path.exists(savepath):\n",
    "    os.mkdir(savepath)\n",
    "model.save(f\"{savepath}/amazon_vec.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualiser des Word Embeddings\n",
    "\n",
    "L'entraînement d'un modèle comme Word2Vec étant non-supervisé, il peut être utile de visualiser les résultats pour se rendre compte de la qualité. Cependant, les Word Embeddings ont certes bien moins de dimensions que les sacs de mots, il n'en reste pas moins qu'ils en comportent souvent plusieurs centaines. \n",
    "\n",
    "Ainsi, pour les visualiser, il est nécessaire de réduire le nombre de dimensions sans que cela n'en change la valeur. Pour cela, on peut faire appel à des techniques de réduction de dimensions telles que PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-44-8d529525dddb>:4: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  X = model[model.wv.vocab]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(32528, 2)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot\n",
    "\n",
    "X = model[model.wv.vocab]\n",
    "pca = PCA(n_components=2)\n",
    "results = pca.fit_transform(X)\n",
    "results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "df = pd.DataFrame(results, columns=['x', 'y'], index = model.wv.vocab)\n",
    "data = df.iloc[:25]\n",
    "\n",
    "data.plot('x', 'y', kind='scatter', ax=ax)\n",
    "\n",
    "for k, v in data.iterrows():\n",
    "    ax.annotate(k, v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autres méthodes de Word Embeddings\n",
    "\n",
    "### GloVe\n",
    "\n",
    "GloVe est un algorithme créé par Penington et al en 2014 faisant suite à Word2Vec tout en étant très différent. Pour un corpus donné, GloVe constitue un vocabulaire ainsi qu'une liste de contexte. Il calcule ensuite la matrice de co-occurrences entre chaque mot du vocabulaire et un contexte donné. La taille du vocabulaire ainsi que des contextes possibles produisent une matrice gigantesque: GloVe utilise donc ensuite différentes méthodes pour réduire le nombre de dimensions de cette matrice, tout en conservant les informations produites.\n",
    "\n",
    "Ainsi, GloVe se distingue de Word2Vec par deux aspects: tout d'abord il repose sur des méthodes statistiques et sur des matrices de co-occurrences plutôt que sur l'entraînement d'un réseau de neurones, et enfin les vecteurs produits ne sont pas de simple vecteurs one-hot, mais des vecteurs de co-occurrences mots-contexte, qui comportent beaucoup plus de valeur sémantique. Contrairement à Word2Vec, GloVe produit une matrice de co-occurrences pour l'ensemble du corpus, tandis que Word2Vec ne traite le contexte que sur une fenêtre donnée. Bien que produire cette matrice peut prendre du temps, l'opération n'est réalisée qu'une seule fois, contrairement à Word2Vec.\n",
    "\n",
    "En cela, la méthode GloVe produit de meilleurs Word Embeddings et ce plus rapidement.\n",
    "\n",
    "### Exemple de matrice de co-occurrences de mots\n",
    "\n",
    "Ref : https://www.linkedin.com/pulse/word2vec-co-occurrence-words-shamane-siriwardhana/\n",
    "\n",
    "<img src='data/img/coocc_matrix.jpeg'>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fastText\n",
    "\n",
    "Bien que Word2Vec et GloVe permettent de produire d'excellents Word Embeddings, ils ont en commun un défaut majeur: ils ne peuvent produire ou donner un vecteur d'un mot qui n'est pas contenu dans leur vocabulaire. En effet lors de l'entraînement, ces deux modèles constituent un vocabulaire pour lequel ils calculent les vecteurs associés à chaque mot. Cependant, si l'on devait utiliser un Word Embeddings produit de cette façon sur un corpus comportant de nouveaux mots, on ne pourrait obtenir les vecteurs pour ces mots: il faudra donc les ignorer. \n",
    "\n",
    "FastText est un algorithme produit par FAIR (Facebook's AI Research) en réponse à cette problématique en 2015. Il repose sur la même méthode que Word2Vec (plus précisémment la méthode Skip-gram). Cependant, contrairement à Word2Vec et GloVe, il ne constitue pas un vocabulaire par unité lexicale mais par sous-unités. Ainsi, au moment de produire le vecteur d'un mot, fastText produit déjà les vecteurs associés à chaque sous-unité composant ce nom, puis les combine. Ainsi pour chaque mot inconnu dans un nouveau corpus, fastText en produira un vecteur en le décomposant automatiquement en sous-unité qu'il connaît. \n",
    "\n",
    "### Exemple des sous-unités lexicales\n",
    "\n",
    "Ref : https://amitness.com/2020/06/fasttext-embeddings/\n",
    "\n",
    "<img src='data/img/fasttext.png'>\n",
    "\n",
    "Un autre avantage énorme de fastText est que l'équipe de FAIR a déjà traité près de 300 langues différentes et a produit des Word Embedding pour chaque d'elle. Ceci vous permet donc d'accéder facilement aux Word Embeddings et de les incorporer à votre projet.\n",
    "\n",
    "## Autres algorithmes de Word Embeddings\n",
    "\n",
    "* ELMo\n",
    "* Pointcare Embeddings\n",
    "* Probabilistic FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utiliser des modèles pré-entraînés\n",
    "\n",
    "De part l'utilisation de réseau de neurones, on considère que Word2Vec et les autres algorithmes similaires font partie du Deep Learning plutôt que du Machine Learning. Les réseaux de neurones ont permis de faire des progrès considérables dans le traitement des données. \n",
    "\n",
    "Cependant, ces progrès se sont fait au coût de modèles très lourds en terme d'espace disque et mémoire, mais aussi au coût de très nombreuses heures d'entraînement. \n",
    "\n",
    "Ainsi dans la communauté de Deep Learning, il est commun de partager son modèle une fois que celui-ci a terminé son entraînement et qu'il y a obtenu des résultats satisfaisants. Ceci est intéréssant pour différentes raisons:\n",
    "\n",
    "* Tout d'abord, cela répond à la difficulté de trouver assez de données pour entraîner un modèle similaire. Si quelqu'un a entraîné un modèle sur un problème similaire au votre, il est logique d'utiliser son modèle plutôt que d'entraîner soi-même un modèle avec moins de données, au risque donc d'obtenir de moins bons résultats.\n",
    "\n",
    "* Entraîner un bon réseau de neurones demande d'avoir une très grande quantité de données. Cependant, la puissance de calcul demandées pour traiter une telle masse de données est bien plus importante que pour les algorithmes en Machine Learning. Ainsi, peut d'ordinateur personnels sont en mesure d'entraîner de bons réseaux sans y passer des jours, voir des semaines. Utiliser un modèle pré-entraîné vous permet d'accéder à ces réseaux sans y invester à nouveau autant de temps.\n",
    "\n",
    "* Un modèle pré-entraîné peut servir de base à votre nouveau modèle. En effet, de manière simpliste, les modèles en Deep Learning peut continuer à apprendre même lorsque leur entraînement est terminé. Ainsi, des modèles peuvent avoir été entraînés sur des données très générales (ex: Wikipédia). Ces modèles sont souvents très grands et ne sont pas très efficaces. Cependant, vous pouvez les utiliser et les affiner sur vos propres données, vous permettant ainsi d'obtenir de meilleurs résultats que si vous aviez créer un modèle depuis le début. On parle alors de **transfer learning**. Nous reviendrons plus tard sur ces notions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemple de Word Embeddings\n",
    "\n",
    "Avant de générer par nous-mêmes des Words Embeddings, nous allons en charger un déjà entraîné afin d'en voir les différents intérêts. Gensim propose certains embeddings déjà prêts que l'on peut directement télécharger, tel que celui entraîné sur le Google News Dataset (environ 3 millions de mots). \n",
    "\n",
    "Attention, ce modèle pèse près de 2gb et peut prendre du temps à télécharger:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=-------------------------------------------------] 2.2% 36.0/1662.8MB downloaded"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-593528365eab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownloader\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mwv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'word2vec-google-news-300'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/cours_dl/lib/python3.8/site-packages/gensim/downloader.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, return_path)\u001b[0m\n\u001b[1;32m    493\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m         \u001b[0m_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    496\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cours_dl/lib/python3.8/site-packages/gensim/downloader.py\u001b[0m in \u001b[0;36m_download\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"{fname}.gz\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0mdst_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m         \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreporthook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_progress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_calculate_md5_checksum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_get_checksum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cours_dl/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m                 \u001b[0mblock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cours_dl/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0;31m# Amount is given, implement using readinto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cours_dl/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    500\u001b[0m         \u001b[0;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    503\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m             \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cours_dl/lib/python3.8/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cours_dl/lib/python3.8/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1239\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1240\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1241\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1242\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cours_dl/lib/python3.8/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1097\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1099\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1100\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Références\n",
    "\n",
    "GloVe : https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "fastText : https://fasttext.cc/\n",
    "\n",
    "Résumés des méthodes de Word Embeddings : http://hunterheidenreich.com/blog/intro-to-word-embeddings/\n",
    "\n",
    "Post de blog sur la différence Word2Vec / fastText : https://amitness.com/2020/06/fasttext-embeddings/\n",
    "\n",
    "https://iksinc.online/tag/continuous-bag-of-words-cbow/\n",
    "\n",
    "https://towardsdatascience.com/nlp-101-word2vec-skip-gram-and-cbow-93512ee24314\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2020/07/neural-networks-from-scratch-in-python-and-r/\n",
    "\n",
    "https://towardsdatascience.com/understanding-neural-networks-19020b758230\n",
    "\n",
    "http://jalammar.github.io/illustrated-word2vec/\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/\n",
    "\n",
    "https://kavita-ganesan.com/comparison-between-cbow-skipgram-subword/#.X0z4CxmxVhE"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOqsFrkTED839+eY31PFksJ",
   "name": "Cours.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
