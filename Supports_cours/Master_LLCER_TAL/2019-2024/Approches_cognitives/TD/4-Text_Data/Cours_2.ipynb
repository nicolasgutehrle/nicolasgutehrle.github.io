{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Données textuelles - Partie 2\n",
    "\n",
    "Dans cette partie du cours, nous allons revenir sur les notions que nous avons vues au cours précédent, en particulier le ``CountVectorizer``, et apprendre comment modifier ses paramètres pour améliorer le traitement des données. Avant cela, nous alons revenir sur certains concepts du Machine-Learning pour expliquer les résultats que nous avons obtenus avec le modèle précédent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appfitting, underfitting, overfitting\n",
    "\n",
    "Entraîner un modèle consiste à fournir des données à un algorithme de tel sorte qu'il apprenne de lui-même par essai-erreur les différents paramètres lui permettant de prédire de nouveaux résultats. \n",
    "\n",
    "Cependant, bien qu'il suffise de fournir les données à l'algorithme, entraîner correctement un modèle n'est pas si simple, et il est très fréquent de créer des modèles qui ne soient pas capables de généraliser. Cette difficulté à généraliser peut s'expliquer de deux manières : soit le modèle n'est pas capable d'apprendre les paramètres à partir des données, soit au contraitre il apprend ces paramètres beaucoup trop bien. Dans le premier cas, le modèle est \"mal-entraîné\" et prédit un résultat au hasard. Dans le second, le modèle a tellement bien appris à partir des données d'entraînement qu'il ne sait pas comment traiter de nouvelles données. \n",
    "\n",
    "En machine-learning, on parle respectivement d'``underfitting`` et d'``overfitting``\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ke2RRF56rP2U"
   },
   "source": [
    "### Underfitting\n",
    "\n",
    "L'underfitting désigne le cas où le modèle n'a pas appris correctement les paramètres à partir des données d'entraînement, et n'est donc pas capable de généraliser. On peut le repérer lorsque les résultats de l'entraînement ne sont pas concluants. \n",
    "\n",
    "En général, les cas d'underfitting sont dus à des données n'ayant pas assez de features. La meilleure façon donc d'y remédier est de transformer les données de telle sorte à obtenir plus de features. Cependant, les cas d'underfitting sont assez rares, précisémment à cause du fait que l'on a en général trop de features.\n",
    "\n",
    "<img src=\"data/img/underfitting.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ruNksZRYrOBS"
   },
   "source": [
    "### Overfitting\n",
    "\n",
    "L'**overfitting** désigne le cas inverse où un modèle apprend beaucoup trop bien les paramètres à partir des données d'entraînement, et n'apprend donc pas à généraliser. D'une certaine façon, le modèle apprend par coeur les caractéristiques des données d'entraînement sans les comprendre réellement. Il est donc incapable de généraliser, et donc de prédire correctement de nouveaux résultats.\n",
    "\n",
    "\n",
    "<img src=\"data/img/overfitting.png\">\n",
    "\n",
    "\n",
    "En général, l'overfitting se détecte lorsque les performances du modèles sont largement supérieures sur les données d'entraînement que sur les données de test.\n",
    "Contrairement à l'underfitting, l'overfitting est un des problèmes les plus souvent rencontrés en machine-learning. Il est généralement dû à un ``manque de données`` par rapport au problème traité ou bien à l'inverse ``un trop grand nombre de features``. Ainsi pour traiter l'overfitting, il faut soit d'obtenir (beaucoup) plus de données d'entraînement, soit de réduire le nombre de features.\n",
    "\n",
    "### Note\n",
    "\n",
    "Attention cependant à ne pas mettre un modèle de côté à cause d'un léger cas d'overfitting. Très souvent, les modèles ont tendance à l'overfit, sans que cela ne soit réellement impactant. Ainsi, on peut dire qu'un modèle généralise correctement si l'écart entre la précision de l'entraînement et du test est inférieur à 10%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appfitting\n",
    "\n",
    "Ainsi, un modèle idéal est un modèle suffisamment complexe pour qu'il soit capable de s'adapter à de nouvelles données, mais ni trop complexe au point de s'adapter parfaitement aux données d'entraînement (overfitting), ni pas assez complexe au point de ne pas s'adapter du tout (underfitting).\n",
    "\n",
    "<img src=\"data/img/appfitting.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z6rGgFt98hSL"
   },
   "source": [
    "## Curse of dimensionality\n",
    "\n",
    "Plus on fournit un grand nombre de données variées, et plus il est facile pour un modèle d'apprendre à généraliser. Pourtant, plus le nombre de dimensions de ces données augmente (donc plus le nombre de features augmente) et plus il est difficile pour un modèle d'identifier les corrélations entre elles. C'est ce que l'on appelle le ``Fléau de la dimension`` (Curse of dimensionality). \n",
    "\n",
    "En effet, plus l'on a de dimensions et plus l'espace entre elles augmente. Ainsi, il devient de plus en plus difficile de voir ce qui les lie. Si l'on reprend le vocabulaire de notre faux corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicolasgutehrle/opt/anaconda3/envs/cours/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amener</th>\n",
       "      <th>anneau</th>\n",
       "      <th>dans</th>\n",
       "      <th>et</th>\n",
       "      <th>gouverner</th>\n",
       "      <th>les</th>\n",
       "      <th>lier</th>\n",
       "      <th>pour</th>\n",
       "      <th>tous</th>\n",
       "      <th>trouver</th>\n",
       "      <th>ténèbres</th>\n",
       "      <th>un</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Un anneau pour les gouverner tous.</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Un anneau pour les trouver.</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Un anneau pour les amener tous et dans les ténèbres les lier.</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    amener  anneau  dans  et  \\\n",
       "Un anneau pour les gouverner tous.                       0       1     0   0   \n",
       "Un anneau pour les trouver.                              0       1     0   0   \n",
       "Un anneau pour les amener tous et dans les ténè...       1       1     1   1   \n",
       "\n",
       "                                                    gouverner  les  lier  \\\n",
       "Un anneau pour les gouverner tous.                          1    1     0   \n",
       "Un anneau pour les trouver.                                 0    1     0   \n",
       "Un anneau pour les amener tous et dans les ténè...          0    3     1   \n",
       "\n",
       "                                                    pour  tous  trouver  \\\n",
       "Un anneau pour les gouverner tous.                     1     1        0   \n",
       "Un anneau pour les trouver.                            1     0        1   \n",
       "Un anneau pour les amener tous et dans les ténè...     1     1        0   \n",
       "\n",
       "                                                    ténèbres  un  \n",
       "Un anneau pour les gouverner tous.                         0   1  \n",
       "Un anneau pour les trouver.                                0   1  \n",
       "Un anneau pour les amener tous et dans les ténè...         1   1  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "corpus = ['Un anneau pour les gouverner tous.', \n",
    "          'Un anneau pour les trouver.', \n",
    "          'Un anneau pour les amener tous et dans les ténèbres les lier.']\n",
    "\n",
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(corpus)\n",
    "\n",
    "vec_df = pd.DataFrame(data = X.toarray(),\n",
    "                        columns = cv.get_feature_names())\n",
    "vec_df.index = corpus\n",
    "vec_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ci-dessus on peut voir que la majorité des cellules contiennt des zéros. En cela pour chaque ligne, très peu de colonne sont réellement pertinentes. Plus notre vocabulaire sera grand et plus ce phénomène s'étendra, et plus d'espace mémoire sera consommé, ce qui a terme peut faire stopper notre programme. \n",
    "\n",
    "Ainsi, bien que plus de features signifie un meilleur entraînement, trop de features peut à terme conduire à de mauvais résultats. Il faut donc pouvoir trouver le bon nombre de features de tel sorte à entraîner correctement notre modèle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QOf_IgX1zCcy"
   },
   "source": [
    "### Feature reduction\n",
    "\n",
    "Une des raisons de l'overfitting est qu'il y a trop de features dans les données d'entraînement (cf. ci-dessus), ce qui fait que les paramètres appris sont beaucoup trop précis. Une solution est donc de réduire le nombre de features. Il existe pour cela des algorithmes tels que PCA capablent de réduire automatiquement le nombre de dimensions. Nous les verrons plus tard lorsque nous aborderons l'apprentissage non-supervisé.\n",
    "\n",
    "Dans notre cas de manipulation textuelle, il est possible de réduire le nombre de features de différentes manières. Lorsque l'on traite des données textuelles, les features utilisées correspondent au vocabulaire utilisé. En plus de préserver la mémoire, il est donc conseillé de réduire la taille de ce vocabulaire, afin que le modèle ne se concentre que sur un ensemble de mots pertinents. Parmi ces méthodes on retrouve en particulier:\n",
    "\n",
    "* Supprimer les mots vides\n",
    "* Changer la taille des n-grammes\n",
    "* Stemmatizer et Lemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prétraiter son corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes déséquilibrées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RI</th>\n",
       "      <th>Na</th>\n",
       "      <th>Mg</th>\n",
       "      <th>Al</th>\n",
       "      <th>Si</th>\n",
       "      <th>K</th>\n",
       "      <th>Ca</th>\n",
       "      <th>Ba</th>\n",
       "      <th>Fe</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.52101</td>\n",
       "      <td>13.64</td>\n",
       "      <td>4.49</td>\n",
       "      <td>1.10</td>\n",
       "      <td>71.78</td>\n",
       "      <td>0.06</td>\n",
       "      <td>8.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.51761</td>\n",
       "      <td>13.89</td>\n",
       "      <td>3.60</td>\n",
       "      <td>1.36</td>\n",
       "      <td>72.73</td>\n",
       "      <td>0.48</td>\n",
       "      <td>7.83</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.51618</td>\n",
       "      <td>13.53</td>\n",
       "      <td>3.55</td>\n",
       "      <td>1.54</td>\n",
       "      <td>72.99</td>\n",
       "      <td>0.39</td>\n",
       "      <td>7.78</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.51766</td>\n",
       "      <td>13.21</td>\n",
       "      <td>3.69</td>\n",
       "      <td>1.29</td>\n",
       "      <td>72.61</td>\n",
       "      <td>0.57</td>\n",
       "      <td>8.22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.51742</td>\n",
       "      <td>13.27</td>\n",
       "      <td>3.62</td>\n",
       "      <td>1.24</td>\n",
       "      <td>73.08</td>\n",
       "      <td>0.55</td>\n",
       "      <td>8.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        RI     Na    Mg    Al     Si     K    Ca   Ba   Fe  Class\n",
       "0  1.52101  13.64  4.49  1.10  71.78  0.06  8.75  0.0  0.0      1\n",
       "1  1.51761  13.89  3.60  1.36  72.73  0.48  7.83  0.0  0.0      1\n",
       "2  1.51618  13.53  3.55  1.54  72.99  0.39  7.78  0.0  0.0      1\n",
       "3  1.51766  13.21  3.69  1.29  72.61  0.57  8.22  0.0  0.0      1\n",
       "4  1.51742  13.27  3.62  1.24  73.08  0.55  8.07  0.0  0.0      1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('data/glass.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit clairement ci-dessous que les 6 classes ne sont pas équilibrées: La majorité des données sont contenues dans les classes 1 et 2, suivi de 7, 3, 5, 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data['Class'].value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicolasgutehrle/opt/anaconda3/envs/cours/lib/python3.9/site-packages/sklearn/utils/validation.py:985: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/nicolasgutehrle/opt/anaconda3/envs/cours/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6851851851851852"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "lr = LogisticRegression()\n",
    "X = data.iloc[:, :-1]\n",
    "y = data.iloc[:, -1:]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = .75)\n",
    "\n",
    "lr = lr.fit(X_train, y_train)\n",
    "lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit ci-dessus que l'on a à peine 62% de précision: l'entraînement ne s'est donc pas bien passé. Bien que ça n'explique pas tout, le fait d'avoir des classes déséquilibrées jouent en grande partie sur le résultat. Il faut donc en priorité résoudre ce problème.\n",
    "\n",
    "Idéalement, pour réequilibrer les classes, ils faut rajouter des données appartenant aux classes lacunaires. Cependant, les données que vous aurez seront en général les seules que vous aurez pu obtenir. Il sera donc difficile de rajouter de nouvelles données. Il faut donc agir au niveau des données que l'on a. Pour cela, nous avons deux options: l'``Under-sampling`` et l'``Over-sampling``\n",
    "\n",
    "En travaillant avec ``scikit-learn``, vous pouvez utiliser la librairie ``imblearn``, qui implémente ces différentes méthodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imblearn\n",
      "  Downloading imblearn-0.0-py2.py3-none-any.whl (1.9 kB)\n",
      "Collecting imbalanced-learn\n",
      "  Downloading imbalanced_learn-0.8.1-py3-none-any.whl (189 kB)\n",
      "\u001b[K     |████████████████████████████████| 189 kB 514 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn>=0.24 in /Users/nicolasgutehrle/opt/anaconda3/envs/cours/lib/python3.9/site-packages (from imbalanced-learn->imblearn) (1.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /Users/nicolasgutehrle/opt/anaconda3/envs/cours/lib/python3.9/site-packages (from imbalanced-learn->imblearn) (1.7.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/nicolasgutehrle/opt/anaconda3/envs/cours/lib/python3.9/site-packages (from imbalanced-learn->imblearn) (1.0.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /Users/nicolasgutehrle/opt/anaconda3/envs/cours/lib/python3.9/site-packages (from imbalanced-learn->imblearn) (1.21.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/nicolasgutehrle/opt/anaconda3/envs/cours/lib/python3.9/site-packages (from scikit-learn>=0.24->imbalanced-learn->imblearn) (2.2.0)\n",
      "Installing collected packages: imbalanced-learn, imblearn\n",
      "Successfully installed imbalanced-learn-0.8.1 imblearn-0.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((214, 9), (214, 1))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imblearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Under-sampling\n",
    "\n",
    "La méthode la plus simple est de réduire le nombre de données à la plus petite classe. On s'assure ainsi d'avoir des classes équilibrées sans en créer de nouvelles. On parle alors d'``Under-sampling``. Il existe plusieurs méthodes d'Under-sampling, la plus simple étant la méthode aléatoire:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nouvelle forme des données:  (54, 9) (54, 1)\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "randomsampler = RandomUnderSampler(random_state=42) # random_state permet de s'assurer que les données seront toujours mélangées de la même manière\n",
    "under_X, under_y = randomsampler.fit_resample(X, y)\n",
    "print('Nouvelle forme des données: ', under_X.shape, under_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAD1CAYAAAB5n7/BAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAKYUlEQVR4nO3cXahl91nH8d/TTFuTtMZCDqU2sZMLWw0qSR1ba0SkKRrT0t4UTKUFAzUX0qa+gEQUgghqoYgRpTC0zYXW9iIGrAo2ghasL7FnkmhexkptY5Ka1FMQ0zZiEvN4cXba6XCS2cnsffZzZj4fGNh7rzV7P38O85111lnrVHcHgLlesOkBAHh2Qg0wnFADDCfUAMMJNcBwQg0w3KF1vOmFF17Yhw8fXsdbA5yRjh079uXu3tpr21pCffjw4Wxvb6/jrQHOSFX178+0zakPgOGEGmA4oQYYTqgBhhNqgOGEGmA4oQYYTqgBhlvLDS/P1+Eb/nzfPuv+33rzvn1Wsr9rS6xv1axvtc7k9a1jbY6oAYYTaoDhhBpgOKEGGE6oAYYTaoDhhBpgOKEGGE6oAYYTaoDhhBpgOKEGGE6oAYYTaoDhhBpgOKEGGE6oAYYTaoDhhBpguKVCXVU/X1X3VtU9VfWxqvqWdQ8GwK5ThrqqXpnk+iRHuvt7kpyT5Jp1DwbArmVPfRxKcm5VHUpyXpL/WN9IAJzolKHu7i8m+UCSB5I8nOS/u/u2k/erquuqaruqtnd2dlY/KcBZaplTHy9L8rYklyT59iTnV9U7T96vu49295HuPrK1tbX6SQHOUsuc+nhTki909053P5Hk1iQ/tN6xAHjaMqF+IMkPVtV5VVVJrkxyfL1jAfC0Zc5R357kliR3JLl78XeOrnkuABYOLbNTd9+Y5MY1zwLAHtyZCDCcUAMMJ9QAwwk1wHBCDTCcUAMMJ9QAwwk1wHBCDTCcUAMMJ9QAwwk1wHBCDTCcUAMMJ9QAwwk1wHBCDTCcUAMMJ9QAwwk1wHBCDTCcUAMMJ9QAwwk1wHBCDTCcUAMMJ9QAwwk1wHBCDTCcUAMMJ9QAwwk1wHBCDTCcUAMMJ9QAwwk1wHBCDTDcUqGuqm+rqluq6l+q6nhVvWHdgwGw69CS+92U5C+6++1V9aIk561xJgBOcMpQV9UFSX4kyU8nSXc/nuTx9Y4FwNOWOfVxSZKdJDdX1Z1V9aGqOv/knarquqrarqrtnZ2dlQ8KcLZaJtSHkrw2yQe7+/IkX0tyw8k7dffR7j7S3Ue2trZWPCbA2WuZUD+U5KHuvn3x/JbshhuAfXDKUHf3I0kerKrXLF66Msl9a50KgK9b9qqP9yb56OKKj88nuXZ9IwFwoqVC3d13JTmy3lEA2Is7EwGGE2qA4YQaYDihBhhOqAGGE2qA4YQaYDihBhhOqAGGE2qA4YQaYDihBhhOqAGGE2qA4YQaYDihBhhOqAGGE2qA4YQaYDihBhhOqAGGE2qA4YQaYDihBhhOqAGGE2qA4YQaYDihBhhOqAGGE2qA4YQaYDihBhhOqAGGE2qA4YQaYDihBhhOqAGGWzrUVXVOVd1ZVX+2zoEA+GbP5Yj6fUmOr2sQAPa2VKir6qIkb07yofWOA8DJlj2i/p0kv5TkqWfaoaquq6rtqtre2dlZxWwAZIlQV9Vbkvxndx97tv26+2h3H+nuI1tbWysbEOBst8wR9RVJ3lpV9yf5eJI3VtUfrnUqAL7ulKHu7l/u7ou6+3CSa5L8VXe/c+2TAZDEddQA4x16Ljt396eSfGotkwCwJ0fUAMMJNcBwQg0wnFADDCfUAMMJNcBwQg0wnFADDCfUAMMJNcBwQg0wnFADDCfUAMMJNcBwQg0wnFADDCfUAMMJNcBwQg0wnFADDCfUAMMJNcBwQg0wnFADDCfUAMMJNcBwQg0wnFADDCfUAMMJNcBwQg0wnFADDCfUAMMJNcBwQg0wnFADDCfUAMOdMtRVdXFV/XVV3VdV91bV+/ZjMAB2HVpinyeT/GJ331FVL01yrKr+srvvW/NsAGSJI+rufri771g8/kqS40leue7BANj1nM5RV9XhJJcnuX2PbddV1XZVbe/s7KxoPACWDnVVvSTJHyf5ue5+9OTt3X20u49095Gtra1VzghwVlsq1FX1wuxG+qPdfet6RwLgRMtc9VFJPpzkeHf/9vpHAuBEyxxRX5HkXUneWFV3Lf5cvea5AFg45eV53f3pJLUPswCwB3cmAgwn1ADDCTXAcEINMJxQAwwn1ADDCTXAcEINMJxQAwwn1ADDCTXAcEINMJxQAwwn1ADDCTXAcEINMJxQAwwn1ADDCTXAcEINMJxQAwwn1ADDCTXAcEINMJxQAwwn1ADDCTXAcEINMJxQAwwn1ADDCTXAcEINMJxQAwwn1ADDCTXAcEINMJxQAwy3VKir6qqq+mxVfa6qblj3UAB8wylDXVXnJPn9JD+R5NIk76iqS9c9GAC7ljmifl2Sz3X357v78SQfT/K29Y4FwNOqu599h6q3J7mqu9+9eP6uJK/v7vectN91Sa5bPH1Nks+uftw9XZjky/v0WZtgfQeb9R1c+722V3X31l4bDq3qE7r7aJKjq3q/ZVXVdncf2e/P3S/Wd7BZ38E1aW3LnPr4YpKLT3h+0eI1APbBMqH+TJLvrKpLqupFSa5J8on1jgXA00556qO7n6yq9yT5ZJJzknyku+9d+2TL2/fTLfvM+g426zu4xqztlD9MBGCz3JkIMJxQAwwn1ADDCfUwVfVdVXVlVb3kpNev2tRMq1RVr6uqH1g8vrSqfqGqrt70XOtQVT+8WN+PbXqW01VVr6+qb108Preqfq2q/rSq3l9VF2x6vtNVVddX1cWn3nMzzqhQV9W1m57hdFTV9Un+JMl7k9xTVSfeqv8bm5lqdarqxiS/m+SDVfWbSX4vyflJbqiqX9nocCtQVf94wuOfye76XprkxjPgl5l9JMlji8c3JbkgyfsXr928qaFW6NeT3F5Vf1NVP1tVe94huCln1FUfVfVAd3/Hpud4vqrq7iRv6O6vVtXhJLck+YPuvqmq7uzuyzc74elZrO+yJC9O8kiSi7r70ao6N8nt3f19m5zvdJ34NaqqzyS5urt3qur8JP/Q3d+72Qmfv6o63t3fvXh8R3e/9oRtd3X3ZRsbbgWq6s4k35/kTUl+MslbkxxL8rEkt3b3VzY43upuId8vVfXPz7Qpycv3c5Y1eEF3fzVJuvv+qvrRJLdU1auyu76D7snu/r8kj1XVv3X3o0nS3f9TVU9teLZVeEFVvSy736lWd+8kSXd/raqe3Oxop+2eqrq2u29O8k9VdaS7t6vq1Ume2PRwK9Dd/VSS25LcVlUvzO5vDH1Hkg8k2egR9oELdXZj/ONJ/uuk1yvJ3+3/OCv1paq6rLvvSpLFkfVbsvtt54E9GjvB41V1Xnc/lt2jlyTJ4hznmRDqC7J7FFZJuqpe0d0PL37ecND/o313kpuq6lez+4uK/r6qHkzy4GLbQfdNX5/ufiK7d2B/oqrO28xI33DgTn1U1YeT3Nzdn95j2x91909tYKyVqKqLsnvU+cge267o7r/dwFgrU1Uv7u7/3eP1C5O8orvv3sBYa7f4h/7y7v7Cpmc5XYsfKF6S3YO8h7r7SxseaSWq6tXd/a+bnuOZHLhQA5xtzqirPgDOREINMJxQAwwn1ADDCTXAcP8PHGMz6MvyPIEAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "under_y['Class'].value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicolasgutehrle/opt/anaconda3/envs/cours/lib/python3.9/site-packages/sklearn/utils/validation.py:985: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/nicolasgutehrle/opt/anaconda3/envs/cours/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr = lr.fit(under_X, under_y)\n",
    "lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une autre méthode proposée par ``imblearn`` est d'utiliser un algorithme KNN pour réduire le nombre de samples. Il existe plusieurs variations de cette méthode, mais de manière générale, l'algorithme supprime tout point qui ne fait pas partie de la même classe que celle de la majorité des voisins:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nouvelle forme des données:  (111, 9) (111, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from imblearn.under_sampling import EditedNearestNeighbours\n",
    "nn_undersampler = EditedNearestNeighbours()\n",
    "under_X, under_y = nn_undersampler.fit_resample(X, y)\n",
    "print('Nouvelle forme des données: ', under_X.shape, under_y.shape)\n",
    "under_y['Class'].value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicolasgutehrle/opt/anaconda3/envs/cours/lib/python3.9/site-packages/sklearn/utils/validation.py:985: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/nicolasgutehrle/opt/anaconda3/envs/cours/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7037037037037037"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr = lr.fit(under_X, under_y)\n",
    "lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Over-sampling\n",
    "\n",
    "On peut également créer des données factices. Celles-ci ressembleront aux données déjà existantes sans être exactement les mêmes. Il existe pour cela différentes méthodes et algorithmes. On parle d'``Over-sampling``.\n",
    "\n",
    "Comme pour l'``Under-sampling``, la méthode la plus simple est de sélectionner aléatoirement des points de données et de les copier. Attention: simplement copier-coller les données de telle sorte à compléter les classes lacunaires n'est pas une solution. En effet, la qualité de l'algorithme dépend de la quantité et de la variété des données. Copier-coller les données ne ferait que rajouter des données que l'algorithme aura déjà vues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nouvelle forme des données:  (456, 9) (456, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "randomoversampler = RandomOverSampler(random_state = 42)\n",
    "over_X, over_y = randomoversampler.fit_resample(X,y)\n",
    "print('Nouvelle forme des données: ', over_X.shape, over_y.shape)\n",
    "over_y['Class'].value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicolasgutehrle/opt/anaconda3/envs/cours/lib/python3.9/site-packages/sklearn/utils/validation.py:985: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/nicolasgutehrle/opt/anaconda3/envs/cours/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7407407407407407"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr = lr.fit(over_X, over_y)\n",
    "lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE\n",
    "\n",
    "Au lieu de copier aléatoirement des points de données comme dans la méthode précédente, ``SMOTE`` génère de nouveaux points de données à partir d'un algorithme de type KNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nouvelle forme des données:  (456, 9) (456, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE()\n",
    "over_X, over_y = smote.fit_resample(X,y)\n",
    "print('Nouvelle forme des données: ', over_X.shape, over_y.shape)\n",
    "over_y['Class'].value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicolasgutehrle/opt/anaconda3/envs/cours/lib/python3.9/site-packages/sklearn/utils/validation.py:985: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/nicolasgutehrle/opt/anaconda3/envs/cours/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6851851851851852"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr = lr.fit(over_X, over_y)\n",
    "lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1IWGG1682wSe"
   },
   "source": [
    "## Supprimer les mots vides\n",
    "\n",
    "Les mots-vides, ou mots-outils, sont tous les mots tels que déterminants, pronoms, prépositions, etc. qui jouent un rôle syntaxique mais ne participent pas au niveau sémantique du texte. Comme décrit par la Loi de Zipf, ce sont les mots que l'on retrouve le plus dans un corpus, et qui par conséquent y sont les moins importants. \n",
    "\n",
    "Puisque la méthode sac de mots ne s'intéresse pas à l'organisation du texte, il est recommandé d'en supprimer les mots vides afin d'en réduire le nombre de dimensions.\n",
    "\n",
    "Comme nous l'avons vu plus tôt, ``CountVectorizer`` prend en charge différents aspects du prétraitement, en particulier la suppression des mots vides. Cependant, il fonctionne par défaut sur l'anglais et ne propose pas de prétraitements pour le français. Ainsi, pour supprimer les mots vides français, il faut lui en fournir une liste, que vous trouverez dans le dossier ``data`` de ce cours. \n",
    "\n",
    "Nous allons dans la cellule ci-dessous charger ce fichier dans une liste, puis le donner en argument à ``CountVectorizer``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 201
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 525,
     "status": "ok",
     "timestamp": 1594303485317,
     "user": {
      "displayName": "Nicolas Gutehrlé",
      "photoUrl": "",
      "userId": "13449761133402795532"
     },
     "user_tz": -120
    },
    "id": "MY2cxcZl2zfJ",
    "outputId": "426696b6-10d9-4071-f21c-6c86353d82a3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'abord',\n",
       " 'absolument',\n",
       " 'afin',\n",
       " 'ah',\n",
       " 'ai',\n",
       " 'aie',\n",
       " 'aient',\n",
       " 'aies',\n",
       " 'ailleurs']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr_stopwords = []\n",
    "with open('data/fr_stopwords.txt') as f:\n",
    "  for line in f:\n",
    "    # on enleve les sauts de lignes et eventuels espaces en trop\n",
    "    line = line.strip()\n",
    "    fr_stopwords.append(line)\n",
    "fr_stopwords[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 166
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 604,
     "status": "ok",
     "timestamp": 1594303611019,
     "user": {
      "displayName": "Nicolas Gutehrlé",
      "photoUrl": "",
      "userId": "13449761133402795532"
     },
     "user_tz": -120
    },
    "id": "y0rgRtJ25Qgi",
    "outputId": "4346ebf0-1a84-4ad2-fd1e-28a71c2246de"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicolasgutehrle/opt/anaconda3/envs/cours/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['quelqu'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/nicolasgutehrle/opt/anaconda3/envs/cours/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amener</th>\n",
       "      <th>anneau</th>\n",
       "      <th>gouverner</th>\n",
       "      <th>lier</th>\n",
       "      <th>trouver</th>\n",
       "      <th>ténèbres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Un anneau pour les gouverner tous.</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Un anneau pour les trouver.</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Un anneau pour les amener tous et dans les ténèbres les lier.</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    amener  anneau  gouverner  \\\n",
       "Un anneau pour les gouverner tous.                       0       1          1   \n",
       "Un anneau pour les trouver.                              0       1          0   \n",
       "Un anneau pour les amener tous et dans les ténè...       1       1          0   \n",
       "\n",
       "                                                    lier  trouver  ténèbres  \n",
       "Un anneau pour les gouverner tous.                     0        0         0  \n",
       "Un anneau pour les trouver.                            0        1         0  \n",
       "Un anneau pour les amener tous et dans les ténè...     1        0         1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer(stop_words= fr_stopwords)\n",
    "X = cv.fit_transform(corpus)\n",
    "\n",
    "vec_df = pd.DataFrame(data = X.toarray(),\n",
    "                        columns = cv.get_feature_names())\n",
    "vec_df.index = corpus\n",
    "vec_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En supprimant les mots-vides, nous avons divisé par deux la taille de notre vocabulaire. Bien qu'ici il ne s'agisse que d'un faux corpus, cela peut avoir un impact considérable sur le temps et la qualité de l'entraînement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y5YY6NOr55CC"
   },
   "source": [
    "## Etendre le contexte avec les n-grammes\n",
    "\n",
    "Comme nous l'avons déjà dit, le sac de mot est du point de vue linguistique uniquement constitué d'un vocabulaire: la syntaxe de la phrase, et l'organisation générale du texte sont totalement mis de côté. Pourtant, un même vocabulaire dans différentes organisations peut donner de multiples sens, comme dans les deux phrases ci-dessous (en anglais):\n",
    "\n",
    "* It's good, it's not so bad\n",
    "* It's bad, it's not so good\n",
    "\n",
    "Avec le sac de mot, nous perdons toutes subtilités apportées par l'organisation de chaque phrase, ce qui est problématique lorsque l'on cherche à traiter du texte comme dans le cas de l'analyse de sentiments.\n",
    "\n",
    "Une solution pour faire face à ce problème tout en conservant la méthode du sac de mots est considérer les ``n-grammes``. \"[Un n-gramme est une sous-séquence de n éléments construite à partir d'une séquence donnée](https://fr.wikipedia.org/wiki/N-gramme)\" (Wikipedia). Jusque là, nous avons tokenizé les phrases au niveau de l'unité lexicale: nous avons donc pris des n-gramme de 1, ou ``unigramme``:\n",
    "\n",
    "``[It, 's, good, it, 's, not, so, bad]``\n",
    "\n",
    "Cependant, nous pouvons sélectionner des n-grammes de la taille n que nous souhaitons, comme des bigrammes (n-grammes de 2) ou trigrammes (n-grammes de 3). Par défaut, les tokens de ``CountVectorizer()`` sont des unigrammes, mais il est possible de choisir la taille des n-grammes en jouant avec le paramètre ``ngram_range``:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-grammes\n",
    "* it's good it's not so bad\n",
    "* unigramme: (it 's good it 's not so bad\n",
    "* bigramme (it's) ('s good) (it 's) ('s not) (not so) (so bad)\n",
    "* trigramme (it's good) ('s good it) (good it 's) (it 's not) ('s not so) (not so bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 650,
     "status": "ok",
     "timestamp": 1594304701314,
     "user": {
      "displayName": "Nicolas Gutehrlé",
      "photoUrl": "",
      "userId": "13449761133402795532"
     },
     "user_tz": -120
    },
    "id": "ED8UP_H47Csv",
    "outputId": "227e3f2a-559e-4ac2-d325-e9a80aaffe2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram Vocabulary : \n",
      "['amener', 'anneau', 'dans', 'et', 'gouverner', 'les', 'lier', 'pour', 'tous', 'trouver', 'ténèbres', 'un']\n",
      "\n",
      "** **\n",
      "\n",
      "Unigram et Bigram Vocabulary : \n",
      "['amener', 'amener tous', 'anneau', 'anneau pour', 'dans', 'dans les', 'et', 'et dans', 'gouverner', 'gouverner tous', 'les', 'les amener', 'les gouverner', 'les lier', 'les trouver', 'les ténèbres', 'lier', 'pour', 'pour les', 'tous', 'tous et', 'trouver', 'ténèbres', 'ténèbres les', 'un', 'un anneau']\n",
      "\n",
      "** **\n",
      "\n",
      "Bigram Vocabulary : \n",
      "['amener tous', 'anneau pour', 'dans les', 'et dans', 'gouverner tous', 'les amener', 'les gouverner', 'les lier', 'les trouver', 'les ténèbres', 'pour les', 'tous et', 'ténèbres les', 'un anneau']\n",
      "\n",
      "** **\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicolasgutehrle/opt/anaconda3/envs/cours/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# ngram_range = (x, y)\n",
    "# ou x est le nombre minimal d'elements à prendre en compte\n",
    "# et y le nombre maximal\n",
    "\n",
    "\n",
    "unigram_vectorizer = CountVectorizer()\n",
    "X = unigram_vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(\"Unigram Vocabulary : \")\n",
    "print(unigram_vectorizer.get_feature_names())\n",
    "\n",
    "print(\"\\n** **\\n\")\n",
    "\n",
    "unibigram_vectorizer = CountVectorizer(ngram_range=(1,2))\n",
    "X = unibigram_vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(\"Unigram et Bigram Vocabulary : \")\n",
    "print(unibigram_vectorizer.get_feature_names())\n",
    "\n",
    "print(\"\\n** **\\n\")\n",
    "\n",
    "bigram_vectorizer = CountVectorizer(ngram_range=(2,2))\n",
    "X = bigram_vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(\"Bigram Vocabulary : \")\n",
    "print(bigram_vectorizer.get_feature_names())\n",
    "\n",
    "print(\"\\n** **\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F5ZpdwU-BWvD"
   },
   "source": [
    "En général, la tokenization se fait au niveau des mots, mais elle est également intéressante au niveau des caractères lorsqu'on travaille par exemple sur un aspect morphologique (détection de la langue, création d'un autre tokenizer, ...). Pour cela, on précise l'analyzer comme étant ``'char'`` dans ``CountVectorizer`` (par défaut, celui-ci est fixé sur ``'word'``):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 537,
     "status": "ok",
     "timestamp": 1594305429889,
     "user": {
      "displayName": "Nicolas Gutehrlé",
      "photoUrl": "",
      "userId": "13449761133402795532"
     },
     "user_tz": -120
    },
    "id": "6ZPJizZoAbeo",
    "outputId": "749741ce-a0f7-44ea-d238-f95cdda6f155"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram Vocabulary : \n",
      "[' ', '.', 'a', 'b', 'd', 'e', 'g', 'i', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'è', 'é']\n",
      "\n",
      "** **\n",
      "\n",
      "Unigram et Bigram Vocabulary : \n",
      "[' ', ' a', ' d', ' e', ' g', ' l', ' p', ' t', '.', 'a', 'am', 'an', 'au', 'b', 'br', 'd', 'da', 'e', 'ea', 'en', 'er', 'es', 'et', 'g', 'go', 'i', 'ie', 'l', 'le', 'li', 'm', 'me', 'n', 'n ', 'ne', 'nn', 'ns', 'nè', 'o', 'ou', 'p', 'po', 'r', 'r ', 'r.', 're', 'rn', 'ro', 's', 's ', 's.', 't', 't ', 'to', 'tr', 'té', 'u', 'u ', 'un', 'ur', 'us', 'uv', 'v', 've', 'è', 'èb', 'é', 'én']\n",
      "\n",
      "** **\n",
      "\n",
      "Bigram Vocabulary : \n",
      "[' a', ' d', ' e', ' g', ' l', ' p', ' t', 'am', 'an', 'au', 'br', 'da', 'ea', 'en', 'er', 'es', 'et', 'go', 'ie', 'le', 'li', 'me', 'n ', 'ne', 'nn', 'ns', 'nè', 'ou', 'po', 'r ', 'r.', 're', 'rn', 'ro', 's ', 's.', 't ', 'to', 'tr', 'té', 'u ', 'un', 'ur', 'us', 'uv', 've', 'èb', 'én']\n",
      "\n",
      "** **\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# analyzer : définie si CountVectorizer doit compter au niveau des mots (par défaut, 'word') ou des caractères ('char')\n",
    "# \n",
    "analyzer = 'char'\n",
    "unigram_vectorizer = CountVectorizer(analyzer = analyzer)\n",
    "X = unigram_vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(\"Unigram Vocabulary : \")\n",
    "print(unigram_vectorizer.get_feature_names())\n",
    "\n",
    "print(\"\\n** **\\n\")\n",
    "\n",
    "unibigram_vectorizer = CountVectorizer(ngram_range=(1,2),\n",
    "                                       analyzer = analyzer)\n",
    "X = unibigram_vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(\"Unigram et Bigram Vocabulary : \")\n",
    "print(unibigram_vectorizer.get_feature_names())\n",
    "\n",
    "print(\"\\n** **\\n\")\n",
    "\n",
    "bigram_vectorizer = CountVectorizer(ngram_range=(2,2), \n",
    "                                    analyzer = analyzer)\n",
    "X = bigram_vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(\"Bigram Vocabulary : \")\n",
    "print(bigram_vectorizer.get_feature_names())\n",
    "\n",
    "print(\"\\n** **\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suivant le problème que vous traitez ou suivant le besoin d'améliorer les résultats obtenu par votre modèle lors de l'entraînement, changer la taille des n-gramme peut s'avérer utile. Cependant, si ici notre corpus est ridicule, prendre des n-grammes trop grands sur de larges corpus risque d'augmenter grandement le nombre de feature, ce qui à terme déteriorera les performances du modèle ainsi que l'usage de la mémoire.\n",
    "\n",
    "Il est donc conseillé d'utiliser des unigrammes, bigrammes, trigrammes, et au plus, des n-grammes de 5 éléments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf - Idf\n",
    "\n",
    "Avec le ``CountVectorizer`` nous avons pu encoder les textes avec la présence ou non d'un mot dans un texte, puis avec son nombre d'occurrence. Bien que la seconde méthode s'est avérée plus pertinente que la première, elle ne nous dit rien de l'importance d'un terme par rapport aux autres dans un même document, et encore moins vis-à-vis des autres documents contenus dans un corpus. En effet, bien qu'un terme puisse apparaître un certain nombre de fois dans un document, il peut ne pas apparaître du tout dans les autre documents du corpus, ce qui le rend encore plus important pour la compréhesion du document actuel.\n",
    "\n",
    "Ainsi, il pourrait être plus pertinent d'encoder le vocabulaire non pas par rapport à leur fréquence d'apparition mais par rapport à leur poids, c'est-à-dire par rapport à leur importance dans le document. Pour cela, nous pouvons utiliser la méthode ``Tf-Idf`` (pour ``Term frequency-Inverse document frequency``), qui repose sur le postulat expliqué ci-dessus. Nous ne reviendrons pas sur l'origine ni sur la façon de le calculer, mais sachez que le TF-IDF est utilisé dans de nombreux domaines du traitement de l'information autre que le Machine-Learning, en particulier les moteurs de recherches.\n",
    "\n",
    "Dans ``scikit-learn``, le calcul du TF-IDF se fait à l'aide de la classe ``TfidfTransformer``. Celui-ci prend en entrée les résultats obtenus par ``CountVectorizer`` avant de les transformer en une matrice contenant les poids de chaque terme:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 201
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 992,
     "status": "ok",
     "timestamp": 1594390584227,
     "user": {
      "displayName": "Nicolas Gutehrlé",
      "photoUrl": "",
      "userId": "13449761133402795532"
     },
     "user_tz": -120
    },
    "id": "ZXBJfvIUBkvk",
    "outputId": "5340f99f-a3ec-49b5-e83e-1991032fe3e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrice de poids :\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amener</th>\n",
       "      <th>anneau</th>\n",
       "      <th>dans</th>\n",
       "      <th>et</th>\n",
       "      <th>gouverner</th>\n",
       "      <th>les</th>\n",
       "      <th>lier</th>\n",
       "      <th>pour</th>\n",
       "      <th>tous</th>\n",
       "      <th>trouver</th>\n",
       "      <th>ténèbres</th>\n",
       "      <th>un</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Un anneau pour les gouverner tous.</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.342496</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.579897</td>\n",
       "      <td>0.342496</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.342496</td>\n",
       "      <td>0.441027</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.342496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Un anneau pour les trouver.</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.381614</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.381614</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.381614</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.646129</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.381614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Un anneau pour les amener tous et dans les ténèbres les lier.</th>\n",
       "      <td>0.320021</td>\n",
       "      <td>0.189010</td>\n",
       "      <td>0.320021</td>\n",
       "      <td>0.320021</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.567029</td>\n",
       "      <td>0.320021</td>\n",
       "      <td>0.189010</td>\n",
       "      <td>0.243384</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.320021</td>\n",
       "      <td>0.189010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      amener    anneau  \\\n",
       "Un anneau pour les gouverner tous.                  0.000000  0.342496   \n",
       "Un anneau pour les trouver.                         0.000000  0.381614   \n",
       "Un anneau pour les amener tous et dans les ténè...  0.320021  0.189010   \n",
       "\n",
       "                                                        dans        et  \\\n",
       "Un anneau pour les gouverner tous.                  0.000000  0.000000   \n",
       "Un anneau pour les trouver.                         0.000000  0.000000   \n",
       "Un anneau pour les amener tous et dans les ténè...  0.320021  0.320021   \n",
       "\n",
       "                                                    gouverner       les  \\\n",
       "Un anneau pour les gouverner tous.                   0.579897  0.342496   \n",
       "Un anneau pour les trouver.                          0.000000  0.381614   \n",
       "Un anneau pour les amener tous et dans les ténè...   0.000000  0.567029   \n",
       "\n",
       "                                                        lier      pour  \\\n",
       "Un anneau pour les gouverner tous.                  0.000000  0.342496   \n",
       "Un anneau pour les trouver.                         0.000000  0.381614   \n",
       "Un anneau pour les amener tous et dans les ténè...  0.320021  0.189010   \n",
       "\n",
       "                                                        tous   trouver  \\\n",
       "Un anneau pour les gouverner tous.                  0.441027  0.000000   \n",
       "Un anneau pour les trouver.                         0.000000  0.646129   \n",
       "Un anneau pour les amener tous et dans les ténè...  0.243384  0.000000   \n",
       "\n",
       "                                                    ténèbres        un  \n",
       "Un anneau pour les gouverner tous.                  0.000000  0.342496  \n",
       "Un anneau pour les trouver.                         0.000000  0.381614  \n",
       "Un anneau pour les amener tous et dans les ténè...  0.320021  0.189010  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(corpus)\n",
    "X = vectorizer.transform(corpus)\n",
    "\n",
    "tfidf = TfidfTransformer()\n",
    "tfidf_X = tfidf.fit_transform(X)\n",
    "print(\"Matrice de poids :\")\n",
    "\n",
    "tfidf_X\n",
    "tfidf_X.toarray()\n",
    "\n",
    "vec_df = pd.DataFrame(data = tfidf_X.toarray(),\n",
    "                        columns = vectorizer.get_feature_names())\n",
    "vec_df.index = corpus\n",
    "vec_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note\n",
    "\n",
    "``TfidfVectorizer`` est une classe regroupant ``CountVectorizer`` et ``TfidfTransformer`` Celui-ci utilise en interne le calcul des fréquences d'apparitions des mots obtenus par ``CountVectorizer`` avant d'effectuer le calcul des poids de chaque terme à l'aide de ``TfidfTransformer``. Ainsi, si vous souhaitez utiliser la méthode Tf-idf dans ``scikit-learn``, il est recommandé d'utiliser le ``TfidfVectorizer`` pour plus de simplicité:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicolasgutehrle/opt/anaconda3/envs/cours/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amener</th>\n",
       "      <th>anneau</th>\n",
       "      <th>dans</th>\n",
       "      <th>et</th>\n",
       "      <th>gouverner</th>\n",
       "      <th>les</th>\n",
       "      <th>lier</th>\n",
       "      <th>pour</th>\n",
       "      <th>tous</th>\n",
       "      <th>trouver</th>\n",
       "      <th>ténèbres</th>\n",
       "      <th>un</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Un anneau pour les gouverner tous.</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.342496</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.579897</td>\n",
       "      <td>0.342496</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.342496</td>\n",
       "      <td>0.441027</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.342496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Un anneau pour les trouver.</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.381614</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.381614</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.381614</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.646129</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.381614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Un anneau pour les amener tous et dans les ténèbres les lier.</th>\n",
       "      <td>0.320021</td>\n",
       "      <td>0.189010</td>\n",
       "      <td>0.320021</td>\n",
       "      <td>0.320021</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.567029</td>\n",
       "      <td>0.320021</td>\n",
       "      <td>0.189010</td>\n",
       "      <td>0.243384</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.320021</td>\n",
       "      <td>0.189010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      amener    anneau  \\\n",
       "Un anneau pour les gouverner tous.                  0.000000  0.342496   \n",
       "Un anneau pour les trouver.                         0.000000  0.381614   \n",
       "Un anneau pour les amener tous et dans les ténè...  0.320021  0.189010   \n",
       "\n",
       "                                                        dans        et  \\\n",
       "Un anneau pour les gouverner tous.                  0.000000  0.000000   \n",
       "Un anneau pour les trouver.                         0.000000  0.000000   \n",
       "Un anneau pour les amener tous et dans les ténè...  0.320021  0.320021   \n",
       "\n",
       "                                                    gouverner       les  \\\n",
       "Un anneau pour les gouverner tous.                   0.579897  0.342496   \n",
       "Un anneau pour les trouver.                          0.000000  0.381614   \n",
       "Un anneau pour les amener tous et dans les ténè...   0.000000  0.567029   \n",
       "\n",
       "                                                        lier      pour  \\\n",
       "Un anneau pour les gouverner tous.                  0.000000  0.342496   \n",
       "Un anneau pour les trouver.                         0.000000  0.381614   \n",
       "Un anneau pour les amener tous et dans les ténè...  0.320021  0.189010   \n",
       "\n",
       "                                                        tous   trouver  \\\n",
       "Un anneau pour les gouverner tous.                  0.441027  0.000000   \n",
       "Un anneau pour les trouver.                         0.000000  0.646129   \n",
       "Un anneau pour les amener tous et dans les ténè...  0.243384  0.000000   \n",
       "\n",
       "                                                    ténèbres        un  \n",
       "Un anneau pour les gouverner tous.                  0.000000  0.342496  \n",
       "Un anneau pour les trouver.                         0.000000  0.381614  \n",
       "Un anneau pour les amener tous et dans les ténè...  0.320021  0.189010  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "vec_df = pd.DataFrame(data = X.toarray(),\n",
    "                        columns = vectorizer.get_feature_names())\n",
    "vec_df.index = corpus\n",
    "vec_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixer la taille maximale du vocabulaire\n",
    "\n",
    "Bien que l'on puisse modifier la taille du vocabulaire en jouant avec différents paramètres tels que les mots-vides ou les n-grammes, la façon la plus simple pour limiter au maximum le nombre de features est de simplement fixé une taille maximale à notre vocabulaire. \n",
    "\n",
    "Les vocabulaires que nous avont créés jusque là sur le ``amazon_review`` contenaient plus de 50 000 mots, pour lesquels seulement une poignée est contenue dans chaque document. Ainsi, on peut choisir de limiter le vocabulaire à 10 000 mots seulement, qui seront les 10 000 mots les plus fréquents de notre corpus. \n",
    "\n",
    "Bien entendu, en procédant de cette manière, on perd la possibilité d'encoder et de représenter le sens d'un grand nombre de documents.  Cependant, on peut à l'inverse se dire que seuls un certain nombre de mots est nécessaire à la compréhension générale de ce corpus. De plus, en procédant ainsi, on limite les risques liés à un trop grand nombre de features (cf. curse of dimensionality).\n",
    "\n",
    "Pour limiter la taille du vocabulaire, on change la valeur du paramètre ``max_features``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicolasgutehrle/opt/anaconda3/envs/cours/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anneau</th>\n",
       "      <th>les</th>\n",
       "      <th>pour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Un anneau pour les gouverner tous.</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Un anneau pour les trouver.</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Un anneau pour les amener tous et dans les ténèbres les lier.</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    anneau  les  pour\n",
       "Un anneau pour les gouverner tous.                       1    1     1\n",
       "Un anneau pour les trouver.                              1    1     1\n",
       "Un anneau pour les amener tous et dans les ténè...       1    3     1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer(max_features=3)\n",
    "X = cv.fit_transform(corpus)\n",
    "\n",
    "vec_df = pd.DataFrame(data = X.toarray(),\n",
    "                        columns = cv.get_feature_names())\n",
    "vec_df.index = corpus\n",
    "vec_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note\n",
    "\n",
    "Comme on peut le voir ci-dessus, le vocabulaire n'est plus que constitué de 3 mots. Parmi ceux-là, on peut voir qu'il y a le mot vide 'les'. Puisque l'on ne garde que les mots les plus fréquents, et que l'on a pas supprimé les mots-vides, ceux-ci apparaîtrons forcément dans notre vocabulaire limité. \n",
    "\n",
    "``Ainsi, lorsqu'on limite la taille du vocabulaire, il ne faut pas oublier également de supprimer les mots vides.``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application des méthodes\n",
    "\n",
    "Nous allons maintenant essayer d'appliquer ces différentes méthodes, séparément puis ensemble, à notre corpus d'analyse de sentiment afin d'essayer d'entraîner un meilleur modèle. Nous allons commencer par charger les données que nous avons préparé au cours précédent, puis créer un dictionnaire ``results`` qui nous permettra de stocker ces différents résultats et de les visualiser:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texts</th>\n",
       "      <th>classes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A déconseiller - Article n'a fonctionné qu'une...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Si vous voulez être déçu achetez le produit ! ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Écran de mauvaise qualité, car il s'use en peu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cet engin ne sert à rien les sons sont pourris...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Très beau produit mais la grue n'a pas fonctio...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               texts  classes\n",
       "0  A déconseiller - Article n'a fonctionné qu'une...        0\n",
       "1  Si vous voulez être déçu achetez le produit ! ...        0\n",
       "2  Écran de mauvaise qualité, car il s'use en peu...        0\n",
       "3  Cet engin ne sert à rien les sons sont pourris...        0\n",
       "4  Très beau produit mais la grue n'a pas fonctio...        0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv('data/binary/bi_as_train.csv')\n",
    "dev = pd.read_csv('data/binary/bi_as_dev.csv')\n",
    "test = pd.read_csv('data/binary/bi_as_test.csv')\n",
    "\n",
    "# on concatène les résultats pour entraîner le vectorizer\n",
    "# sur l'ensemble du corpus\n",
    "data = pd.concat([train['texts'], dev['texts'], test['texts']])\n",
    "\n",
    "# dictionnaire dans lequel on va stocker les performances \n",
    "# de chaque modèle\n",
    "results = {}\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chargement du modèle précédent\n",
    "Afin de pouvoir comparer les résultats des différentes méthodes, nous allons charger notre modèle précédent ainsi que le vectorize associé:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import load\n",
    "cv1 = load('data/logreg_model/vectorizer.joblib')\n",
    "model = load('data/logreg_model/model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forme de la matrice : (4000, 48768)\n",
      "Forme de la matrice : (4000, 48768)\n",
      "Score : 0.88425\n"
     ]
    }
   ],
   "source": [
    "X_dev, y_dev = cv1.transform(dev['texts']), dev['classes']\n",
    "print(\"Forme de la matrice :\", X_dev.shape)\n",
    "\n",
    "X_test, y_test = cv1.transform(test['texts']), test['classes']\n",
    "print(\"Forme de la matrice :\", X_test.shape)\n",
    "\n",
    "score = model.score(X_dev, y_dev)\n",
    "results['cv_1'] = score\n",
    "print(\"Score :\", score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88425"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = model.score(X_dev, y_dev)\n",
    "score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.886"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV2 (CountVectorizer en supprimant les mots vides) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicolasgutehrle/opt/anaconda3/envs/cours/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['quelqu'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forme de la matrice : (160000, 48199)\n",
      "Forme de la matrice : (4000, 48199)\n",
      "Forme de la matrice : (4000, 48199)\n",
      "Score : 0.8535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicolasgutehrle/opt/anaconda3/envs/cours/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "cv2 = CountVectorizer(stop_words=fr_stopwords)\n",
    "\n",
    "# fit_transform regroupe les deux étapes fit et transform en une seule\n",
    "cv2.fit(data) \n",
    "\n",
    "X_train, y_train = cv2.transform(train['texts']), train['classes']\n",
    "print(\"Forme de la matrice :\", X_train.shape)\n",
    "\n",
    "X_dev, y_dev = cv2.transform(dev['texts']), dev['classes']\n",
    "print(\"Forme de la matrice :\", X_dev.shape)\n",
    "\n",
    "X_test, y_test = cv2.transform(test['texts']), test['classes']\n",
    "print(\"Forme de la matrice :\", X_test.shape)\n",
    "\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "score = lr.score(X_dev, y_dev)\n",
    "results['cv2'] = score\n",
    "print(\"Score :\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8555"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV3 (CountVectorizer en prenant des bigrammes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forme de la matrice : (160000, 827766)\n",
      "Forme de la matrice : (4000, 827766)\n",
      "Forme de la matrice : (4000, 827766)\n",
      "Score : 0.90975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicolasgutehrle/opt/anaconda3/envs/cours/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "cv3 = CountVectorizer(ngram_range=(2,2))\n",
    "\n",
    "# fit_transform regroupe les deux étapes fit et transform en une seule\n",
    "cv3.fit(data) \n",
    "\n",
    "X_train, y_train = cv3.transform(train['texts']), train['classes']\n",
    "print(\"Forme de la matrice :\", X_train.shape)\n",
    "\n",
    "X_dev, y_dev = cv3.transform(dev['texts']), dev['classes']\n",
    "print(\"Forme de la matrice :\", X_dev.shape)\n",
    "\n",
    "X_test, y_test = cv3.transform(test['texts']), test['classes']\n",
    "print(\"Forme de la matrice :\", X_test.shape)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "score = lr.score(X_dev, y_dev)\n",
    "results['cv3'] = score\n",
    "print(\"Score :\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.899"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF1 (Tf-idf seul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forme de la matrice : (160000, 48768)\n",
      "Forme de la matrice : (4000, 48768)\n",
      "Forme de la matrice : (4000, 48768)\n",
      "Score : 0.89125\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tf1 = TfidfVectorizer()\n",
    "tf1.fit(data) \n",
    "\n",
    "X_train, y_train = tf1.transform(train['texts']), train['classes']\n",
    "print(\"Forme de la matrice :\", X_train.shape)\n",
    "\n",
    "X_dev, y_dev = tf1.transform(dev['texts']), dev['classes']\n",
    "print(\"Forme de la matrice :\", X_dev.shape)\n",
    "\n",
    "X_test, y_test = tf1.transform(test['texts']), test['classes']\n",
    "print(\"Forme de la matrice :\", X_test.shape)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "score = lr.score(X_dev, y_dev)\n",
    "results['tf1'] = score\n",
    "print(\"Score :\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89275"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF2 (Tf-idf en supprimant les mots vides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicolasgutehrle/opt/anaconda3/envs/cours/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['quelqu'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forme de la matrice : (160000, 48199)\n",
      "Forme de la matrice : (4000, 48199)\n",
      "Forme de la matrice : (4000, 48199)\n",
      "Score : 0.86125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicolasgutehrle/opt/anaconda3/envs/cours/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "tf2 = TfidfVectorizer(stop_words=fr_stopwords)\n",
    "tf2.fit(data) \n",
    "\n",
    "\n",
    "X_train, y_train = tf2.transform(train['texts']), train['classes']\n",
    "print(\"Forme de la matrice :\", X_train.shape)\n",
    "\n",
    "X_dev, y_dev = tf2.transform(dev['texts']), dev['classes']\n",
    "print(\"Forme de la matrice :\", X_dev.shape)\n",
    "\n",
    "X_test, y_test = tf2.transform(test['texts']), test['classes']\n",
    "print(\"Forme de la matrice :\", X_test.shape)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "score = lr.score(X_dev, y_dev)\n",
    "results['tf2'] = score\n",
    "print('Score :', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.856"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF3 (Tf-idf en supprimant les mots-vides et prenant des bigrammes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicolasgutehrle/opt/anaconda3/envs/cours/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['quelqu'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forme de la matrice : (160000, 1016363)\n",
      "Forme de la matrice : (4000, 1016363)\n",
      "Forme de la matrice : (4000, 1016363)\n",
      "Score : 0.79\n"
     ]
    }
   ],
   "source": [
    "tf3 = TfidfVectorizer(stop_words=fr_stopwords,\n",
    "                            ngram_range=(2,2))\n",
    "tf3.fit(data) \n",
    "\n",
    "\n",
    "X_train, y_train = tf3.transform(train['texts']), train['classes']\n",
    "print(\"Forme de la matrice :\", X_train.shape)\n",
    "\n",
    "X_dev, y_dev = tf3.transform(dev['texts']), dev['classes']\n",
    "print(\"Forme de la matrice :\", X_dev.shape)\n",
    "\n",
    "X_test, y_test = tf3.transform(test['texts']), test['classes']\n",
    "print(\"Forme de la matrice :\", X_test.shape)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "score = lr.score(X_dev, y_dev)\n",
    "results['tf3'] = score\n",
    "print('Score :', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7945"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF4 (Tf-idf en supprimant les mots-vides et limitant le vocabulaire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicolasgutehrle/opt/anaconda3/envs/cours/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['quelqu'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forme de la matrice : (160000, 10000)\n",
      "Forme de la matrice : (4000, 10000)\n",
      "Forme de la matrice : (4000, 10000)\n",
      "Score : 0.86175\n"
     ]
    }
   ],
   "source": [
    "tf4 = TfidfVectorizer(stop_words=fr_stopwords,\n",
    "                            max_features= 10000)\n",
    "\n",
    "# fit_transform regroupe les deux étapes fit et transform en une seule\n",
    "tf4.fit(data) \n",
    "\n",
    "\n",
    "X_train, y_train = tf4.transform(train['texts']), train['classes']\n",
    "print(\"Forme de la matrice :\", X_train.shape)\n",
    "\n",
    "X_dev, y_dev = tf4.transform(dev['texts']), dev['classes']\n",
    "print(\"Forme de la matrice :\", X_dev.shape)\n",
    "\n",
    "X_test, y_test = tf4.transform(test['texts']), test['classes']\n",
    "print(\"Forme de la matrice :\", X_test.shape)\n",
    "\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "score = lr.score(X_dev, y_dev)\n",
    "results['tf4'] = score\n",
    "print(\"Score :\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85825"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation des performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "viz = pd.Series(results)\n",
    "viz.plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec le graphique ci-dessus, on peut voir que les meilleurs résultats sont d'abord obtenus par CV3 (CountVectorizer en prenant des bigrammes), suivis de près par TF1 (Tf-idf sans aucun  changement paramètre).\n",
    "\n",
    "A l'inverse, les plus mauvais résultats sont obtenus par TF3 (Tf-idf en supprimant les mots vides et en prenant les bigrammes), suivis par CV2 (CountVectorizer en supprimant seulement les mots-vides). \n",
    "\n",
    "On se rend donc bien compte que jouer sur les différentes méthodes d'encodage du vocabulaire ainsi que de jouer avec leurs paramètres peut avoir une grande influence sur les performances de nos modèles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4boc2cUY8nND"
   },
   "source": [
    "## Pipeline\n",
    "\n",
    "\n",
    "Une fois un modèle entraîné, il est important de le sauvegarder sur le disque afin de conserver l'entraînement ainsi que pour l'utiliser dans d'autres programmes. Cependant, il est important de sauvegarder tout outil de prétraitement (comme un vectorizer) utilisé pendant la phase d'entraînement afin d'appliquer les mêmes traitements à ces nouvelles données.\n",
    "\n",
    "Précédemment, nous avons sauvegardé notre modèle ainsi que son vectorizer sur le disque. Cependant, ceci peut s'avérer laborieux lors de la mise en production puisqu'il faut appliquer la même chaîne de traitements que lors de l'entraînement. Pour faciliter cette étape, il est possible de créer à l'aide de ``scikit-learn`` une ``Pipeline``: celle-ci permet d'associer les différentes étapes de prétraitement ainsi que le modèle au sein d'un seul et même objet. \n",
    "\n",
    "Cet objet ``Pipeline`` se comporte comme un modèle et peut donc s'entraîner sur des données avant d'être utilisé pour prédire de nouveaux résultats. A la différence des autres modèles, il appliquera directement tout prétraitement qu'on le lui aura ajouté, tel qu'un vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 182
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11018,
     "status": "ok",
     "timestamp": 1594384937379,
     "user": {
      "displayName": "Nicolas Gutehrlé",
      "photoUrl": "",
      "userId": "13449761133402795532"
     },
     "user_tz": -120
    },
    "id": "Ry9MHxUK9sJU",
    "outputId": "0c51f1fb-5f32-417a-de9c-cbd3de9f9e8a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicolasgutehrle/opt/anaconda3/envs/cours/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score : 0.90975\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from joblib import dump\n",
    "\n",
    "# ici on se contente simplement de renommer les variables\n",
    "# sans les faire passer par CountVectorizer\n",
    "X_train, y_train = train['texts'], train['classes']\n",
    "X_dev, y_dev = dev['texts'], dev['classes']\n",
    "\n",
    "# Pipeline prend en argument une liste de tuple\n",
    "# Le 1er element du tuple est le nom de l'objet, le 2nd est l'objet lui-meme\n",
    "pipe = Pipeline(\n",
    "    [\n",
    "     ('vectorizer', CountVectorizer(ngram_range=(2,2))), # 1ere couche : vectorizer\n",
    "     ('linear', LogisticRegression()) # Derniere couche : modele\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# on entraine le Pipeline comme si l'on entrainait un modele\n",
    "pipe.fit(X_train, y_train)\n",
    "score = pipe.score(X_dev, y_dev)\n",
    "print(\"Score :\", score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut ensuite sauvegarder la Pipeline sur le disque comme précédemment, à la différence que l'on ne sauvegarde qu'un seul objet cette fois:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/pipe_model/model.joblib']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump\n",
    "import os\n",
    "os.mkdir('data/pipe_model')\n",
    "dump(pipe, 'data/pipe_model/model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Références\n",
    "\n",
    "Le fichier d'origine de mots vides français a été obtenu ici: https://lionbridge.ai/datasets/20-best-french-language-datasets-for-machine-learning/\n",
    "\n",
    "### Documentation: \n",
    "* Pipeline : https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('cours')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "63d2c08867e999184b706abc1228e6e14a103e818b92fa06feeba82a11accc73"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
